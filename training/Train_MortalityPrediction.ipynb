{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "djRNEwzvcNO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/project2/AI_agent_train_sepsis.csv'\n",
        "\n",
        "data = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "GUOI9mKRcLVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# mortality_90d 컬럼의 값 분포 확인\n",
        "if 'mortality_90d' in data.columns:\n",
        "    value_counts = data['mortality_90d'].value_counts()\n",
        "    print(\"mortality_90d 값 분포:\")\n",
        "    print(value_counts)\n",
        "    print(\"\\n비율:\")\n",
        "    print(data['mortality_90d'].value_counts(normalize=True) * 100, \"%\")\n",
        "else:\n",
        "    print(\"mortality_90d 컬럼이 데이터셋에 존재하지 않습니다.\")"
      ],
      "metadata": {
        "id": "i8EWMqZrmC_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split, GroupKFold\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, f1_score, precision_recall_curve\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nB4TN_8ccpYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_patient_data(df):\n",
        "    numerical_cols = [col for col in df.columns if col not in ['mortality_90d', 'icustayid', 'charttime', 'bloc']]\n",
        "\n",
        "    agg_dict = {\n",
        "        'mortality_90d': 'first',\n",
        "    }\n",
        "\n",
        "    for col in numerical_cols:\n",
        "        if col == 'age':\n",
        "            agg_dict[col] = 'first'\n",
        "        else:\n",
        "            agg_dict[col] = ['mean', 'min', 'max', 'std', 'last']\n",
        "\n",
        "    agg_dict['charttime'] = 'count'\n",
        "\n",
        "    agg_df = df.groupby('icustayid').agg(agg_dict)\n",
        "\n",
        "    agg_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in agg_df.columns]\n",
        "\n",
        "    return agg_df\n",
        "\n",
        "patient_df = aggregate_patient_data(data)\n",
        "\n",
        "def clean_feature_names(df):\n",
        "    clean_df = df.copy()\n",
        "\n",
        "    clean_columns = {}\n",
        "    for col in clean_df.columns:\n",
        "        new_col = re.sub(r'[^\\w\\s_]', '', col)\n",
        "        new_col = re.sub(r'\\s+', '_', new_col)\n",
        "        clean_columns[col] = new_col\n",
        "\n",
        "    clean_df = clean_df.rename(columns=clean_columns)\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "patient_df = clean_feature_names(patient_df)\n",
        "\n",
        "X = patient_df.drop('mortality_90d_first', axis=1)\n",
        "y = patient_df['mortality_90d_first']\n",
        "\n",
        "\n",
        "patient_ids = patient_df.index.unique().tolist()\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(patient_ids)\n",
        "train_size = int(len(patient_ids) * 0.8)\n",
        "train_ids = patient_ids[:train_size]\n",
        "test_ids = patient_ids[train_size:]\n",
        "\n",
        "X_train = X.loc[train_ids]\n",
        "X_test = X.loc[test_ids]\n",
        "y_train = y.loc[train_ids]\n",
        "y_test = y.loc[test_ids]"
      ],
      "metadata": {
        "id": "8tvS8yaAdVcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_df.columns[10:30]"
      ],
      "metadata": {
        "id": "JobdVQmni48E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from catboost import CatBoostClassifier, Pool\n",
        "# from sklearn.model_selection import train_test_split, GroupKFold\n",
        "# from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, f1_score, precision_recall_curve\n",
        "# import lightgbm as lgb\n",
        "# import xgboost as xgb\n",
        "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import re\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import shap  # Add SHAP for feature importance\n",
        "\n",
        "# # Assuming the rest of your code remains the same until the model definitions and training\n",
        "# # Read CSV file, prepare data, etc.\n",
        "\n",
        "# print(\"Defining models...\")\n",
        "\n",
        "# # CatBoost\n",
        "# catboost_model = CatBoostClassifier(\n",
        "#     iterations=500,\n",
        "#     depth=6,\n",
        "#     learning_rate=0.05,\n",
        "#     verbose=0,\n",
        "#     class_weights={0: 1, 1: 10}\n",
        "# )\n",
        "\n",
        "# # LightGBM\n",
        "# lgb_model = lgb.LGBMClassifier(\n",
        "#     n_estimators=500,\n",
        "#     num_leaves=62,\n",
        "#     learning_rate=0.05,\n",
        "#     subsample=0.8,\n",
        "#     colsample_bytree=0.8,\n",
        "#     scale_pos_weight=4,\n",
        "#     verbose=-1\n",
        "# )\n",
        "\n",
        "# # XGBoost\n",
        "# xgb_model = xgb.XGBClassifier(\n",
        "#     n_estimators=500,\n",
        "#     max_depth=6,\n",
        "#     learning_rate=0.05,\n",
        "#     subsample=0.8,\n",
        "#     colsample_bytree=0.8,\n",
        "#     scale_pos_weight=10,\n",
        "#     verbosity=0\n",
        "# )\n",
        "\n",
        "# # RandomForest\n",
        "# rf_model = RandomForestClassifier(\n",
        "#     n_estimators=200,\n",
        "#     max_depth=12,\n",
        "#     min_samples_split=10,\n",
        "#     class_weight='balanced',\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "# # for name, model in [('LightGBM', lgb_model),\n",
        "# #                     '''('XGBoost', xgb_model), ('RandomForest', rf_model), ('CatBoost', catboost_model)''']:\n",
        "# print(\"Training and evaluating individual models:\")\n",
        "# for name, model in [('LightGBM', lgb_model),\n",
        "#                     ]:\n",
        "#     print(f\"Training {name} model...\")\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "#     auc = roc_auc_score(y_test, y_pred_proba)\n",
        "#     print(f\"{name} AUC: {auc:.4f}\")\n",
        "\n",
        "# # Stacking ensemble\n",
        "# print(\"Defining stacking ensemble model...\")\n",
        "# stacking_model = StackingClassifier(\n",
        "#     estimators=[\n",
        "#         ('catboost', catboost_model),\n",
        "#         ('lightgbm', lgb_model),\n",
        "#         ('xgboost', xgb_model),\n",
        "#         ('randomforest', rf_model)\n",
        "#     ],\n",
        "#     final_estimator=LogisticRegression(class_weight='balanced', C=0.5, max_iter=1000),\n",
        "#     cv=5,\n",
        "#     n_jobs=-1,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# print(\"Training stacking ensemble model...\")\n",
        "# stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate models (assuming this part is the same)\n",
        "# models = {\n",
        "#     'CatBoost': catboost_model,\n",
        "#     'LightGBM': lgb_model,\n",
        "#     'XGBoost': xgb_model,\n",
        "#     'RandomForest': rf_model,\n",
        "#     'Stacking': stacking_model\n",
        "# }\n",
        "\n",
        "# # This will store results from your evaluate_model function\n",
        "# results = {}\n",
        "\n"
      ],
      "metadata": {
        "id": "mm_QCRhZJypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from catboost import CatBoostClassifier, Pool\n",
        "# from sklearn.model_selection import train_test_split, GroupKFold\n",
        "# from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, f1_score, precision_recall_curve\n",
        "# import lightgbm as lgb\n",
        "# import xgboost as xgb\n",
        "# from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import re\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import shap # Add SHAP for feature importance\n",
        "\n",
        "# # Assuming the rest of your code remains the same until the model definitions and training\n",
        "# # Read CSV file, prepare data, etc.\n",
        "\n",
        "# print(\"Defining models...\")\n",
        "\n",
        "# # CatBoost\n",
        "# catboost_model = CatBoostClassifier(\n",
        "#     iterations=500,\n",
        "#     depth=6,\n",
        "#     learning_rate=0.05,\n",
        "#     verbose=0,\n",
        "#     class_weights={0: 1, 1: 10}\n",
        "# )\n",
        "\n",
        "# # LightGBM\n",
        "# lgb_model = lgb.LGBMClassifier(\n",
        "#     n_estimators=500,\n",
        "#     num_leaves=62,\n",
        "#     learning_rate=0.05,\n",
        "#     subsample=0.8,\n",
        "#     colsample_bytree=0.8,\n",
        "#     scale_pos_weight=10,\n",
        "#     verbose=-1\n",
        "# )\n",
        "\n",
        "# # XGBoost\n",
        "# xgb_model = xgb.XGBClassifier(\n",
        "#     n_estimators=500,\n",
        "#     max_depth=6,\n",
        "#     learning_rate=0.05,\n",
        "#     subsample=0.8,\n",
        "#     colsample_bytree=0.8,\n",
        "#     scale_pos_weight=10,\n",
        "#     verbosity=0\n",
        "# )\n",
        "\n",
        "# # RandomForest\n",
        "# rf_model = RandomForestClassifier(\n",
        "#     n_estimators=200,\n",
        "#     max_depth=12,\n",
        "#     min_samples_split=10,\n",
        "#     class_weight='balanced',\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "# # for name, model in [('CatBoost', catboost_model), ('LightGBM', lgb_model),\n",
        "# #                     ('XGBoost', xgb_model), ('RandomForest', rf_model)]:\n",
        "# print(\"Training and evaluating individual models:\")\n",
        "# for name, model in [('LightGBM', lgb_model),\n",
        "#                     ]:\n",
        "#     print(f\"Training {name} model...\")\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "#     auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "#     # 추가: F1 점수 계산 및 출력\n",
        "#     # 최적의 임계값 찾기 (F1 점수 기준)\n",
        "#     precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "#     f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "#     optimal_idx = np.argmax(f1_scores)\n",
        "#     optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "#     # 최적 임계값을 적용한 예측 및 F1 점수 계산\n",
        "#     y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "#     f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "#     # optimal_idx에 해당하는 precision과 recall 값 가져오기\n",
        "#     optimal_precision = precision[optimal_idx]\n",
        "#     optimal_recall = recall[optimal_idx]\n",
        "\n",
        "#     print(f\"{name} AUC: {auc:.4f}, F1 Score: {f1:.4f}, Precision: {optimal_precision:.4f}, Recall: {optimal_recall:.4f}, Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# # 각 모델 평가\n",
        "# results = {}\n",
        "# for name, model in models.items():\n",
        "#     print(f\"Evaluating {name} model...\")\n",
        "#     results[name] = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# # AUC 기준 모델 성능 비교\n",
        "# print(\"\\nModel performance comparison (sorted by AUC):\")\n",
        "# for name, metrics in sorted(results.items(), key=lambda x: x[1]['AUC'], reverse=True):\n",
        "#     print(f\"{name}: AUC = {metrics['AUC']:.4f}, F1 Score = {metrics['F1 Score']:.4f}, Threshold = {metrics['Optimal Threshold']:.4f}\")\n",
        "\n",
        "# # F1 점수 기준 모델 성능 비교\n",
        "# print(\"\\nModel performance comparison (sorted by F1 Score):\")\n",
        "# for name, metrics in sorted(results.items(), key=lambda x: x[1]['F1 Score'], reverse=True):\n",
        "#     print(f\"{name}: F1 Score = {metrics['F1 Score']:.4f}, AUC = {metrics['AUC']:.4f}, Threshold = {metrics['Optimal Threshold']:.4f}\")\n",
        "\n",
        "# # AUC 및 F1 점수 시각화\n",
        "# plt.figure(figsize=(12, 6))\n",
        "\n",
        "# # AUC 비교\n",
        "# plt.subplot(1, 2, 1)\n",
        "# auc_values = [metrics['AUC'] for name, metrics in results.items()]\n",
        "# model_names = list(results.keys())\n",
        "\n",
        "# sns.barplot(x=model_names, y=auc_values)\n",
        "# plt.title('AUC Comparison')\n",
        "# plt.ylim(0.7, 1.0)\n",
        "# plt.xticks(rotation=45)\n",
        "\n",
        "# # F1 Score 비교\n",
        "# plt.subplot(1, 2, 2)\n",
        "# f1_values = [metrics['F1 Score'] for name, metrics in results.items()]\n",
        "\n",
        "# sns.barplot(x=model_names, y=f1_values)\n",
        "# plt.title('F1 Score Comparison')\n",
        "# plt.ylim(0, 1.0)\n",
        "# plt.xticks(rotation=45)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_lymvSw1jj9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split, GroupKFold\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, f1_score, precision_recall_curve\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap # Add SHAP for feature importance\n",
        "\n",
        "# Assuming the rest of your code remains the same until the model definitions and training\n",
        "# Read CSV file, prepare data, etc.\n",
        "\n",
        "print(\"Defining models...\")\n",
        "\n",
        "# CatBoost\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    verbose=0,\n",
        "    class_weights={0: 1, 1: 4}\n",
        ")\n",
        "\n",
        "# LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    num_leaves=70,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=4,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=4,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# RandomForest\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=12,\n",
        "    min_samples_split=10,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "# for name, model in [('CatBoost', catboost_model), ('LightGBM', lgb_model),\n",
        "#                     ('XGBoost', xgb_model), ('RandomForest', rf_model)]:\n",
        "print(\"Training and evaluating individual models:\")\n",
        "for name, model in [('CatBoost', catboost_model), ('LightGBM', lgb_model),\n",
        "                    ('XGBoost', xgb_model), ('RandomForest', rf_model)]:\n",
        "    print(f\"Training {name} model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    # 추가: F1 점수 계산 및 출력\n",
        "    # 최적의 임계값 찾기 (F1 점수 기준)\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "    # 최적 임계값을 적용한 예측 및 F1 점수 계산\n",
        "    y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # optimal_idx에 해당하는 precision과 recall 값 가져오기\n",
        "    optimal_precision = precision[optimal_idx]\n",
        "    optimal_recall = recall[optimal_idx]\n",
        "\n",
        "    print(f\"{name} AUC: {auc:.4f}, F1 Score: {f1:.4f}, Precision: {optimal_precision:.4f}, Recall: {optimal_recall:.4f}, Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Stacking ensemble\n",
        "print(\"Defining stacking ensemble model...\")\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('catboost', catboost_model),\n",
        "        ('lightgbm', lgb_model),\n",
        "        ('xgboost', xgb_model),\n",
        "        ('randomforest', rf_model)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(class_weight='balanced', C=0.5, max_iter=1000),\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training stacking ensemble model...\")\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# 스태킹 모델 평가 및 F1 점수 계산\n",
        "y_pred_stack_proba = stacking_model.predict_proba(X_test)[:, 1]\n",
        "stack_auc = roc_auc_score(y_test, y_pred_stack_proba)\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_stack_proba)\n",
        "f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "y_pred_stack = (y_pred_stack_proba >= optimal_threshold).astype(int)\n",
        "stack_f1 = f1_score(y_test, y_pred_stack)\n",
        "\n",
        "print(f\"Stacking ensemble AUC: {stack_auc:.4f}, F1 Score: {stack_f1:.4f}, Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(\"Stacking ensemble classification report:\")\n",
        "print(classification_report(y_test, y_pred_stack))\n",
        "\n",
        "# 모델 평가 함수 정의\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "    y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        'AUC': auc,\n",
        "        'F1 Score': f1,\n",
        "        'Optimal Threshold': optimal_threshold,\n",
        "        'Predictions': y_pred,\n",
        "        'Probabilities': y_pred_proba\n",
        "    }\n",
        "\n",
        "models = {\n",
        "    'CatBoost': catboost_model,\n",
        "    'LightGBM': lgb_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'RandomForest': rf_model,\n",
        "    'Stacking': stacking_model\n",
        "}\n",
        "\n",
        "# 각 모델 평가\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Evaluating {name} model...\")\n",
        "    results[name] = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# AUC 기준 모델 성능 비교\n",
        "print(\"\\nModel performance comparison (sorted by AUC):\")\n",
        "for name, metrics in sorted(results.items(), key=lambda x: x[1]['AUC'], reverse=True):\n",
        "    print(f\"{name}: AUC = {metrics['AUC']:.4f}, F1 Score = {metrics['F1 Score']:.4f}, Threshold = {metrics['Optimal Threshold']:.4f}\")\n",
        "\n",
        "# F1 점수 기준 모델 성능 비교\n",
        "print(\"\\nModel performance comparison (sorted by F1 Score):\")\n",
        "for name, metrics in sorted(results.items(), key=lambda x: x[1]['F1 Score'], reverse=True):\n",
        "    print(f\"{name}: F1 Score = {metrics['F1 Score']:.4f}, AUC = {metrics['AUC']:.4f}, Threshold = {metrics['Optimal Threshold']:.4f}\")\n",
        "\n",
        "# AUC 및 F1 점수 시각화\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# AUC 비교\n",
        "plt.subplot(1, 2, 1)\n",
        "auc_values = [metrics['AUC'] for name, metrics in results.items()]\n",
        "model_names = list(results.keys())\n",
        "\n",
        "sns.barplot(x=model_names, y=auc_values)\n",
        "plt.title('AUC Comparison')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# F1 Score 비교\n",
        "plt.subplot(1, 2, 2)\n",
        "f1_values = [metrics['F1 Score'] for name, metrics in results.items()]\n",
        "\n",
        "sns.barplot(x=model_names, y=f1_values)\n",
        "plt.title('F1 Score Comparison')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fmpVIHTadqPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    f1_scores = 2 * precision_curve * recall_curve / (precision_curve + recall_curve + 1e-7)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "    # 최적 임계값에서의 예측\n",
        "    y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    # 최적 임계값에서의 정밀도, 재현율, F1 점수\n",
        "    optimal_precision = precision_curve[optimal_idx]\n",
        "    optimal_recall = recall_curve[optimal_idx]\n",
        "    optimal_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        'AUC': auc,\n",
        "        'F1 Score': optimal_f1,\n",
        "        'Optimal Threshold': optimal_threshold,\n",
        "        'Predictions': y_pred,\n",
        "        'Probabilities': y_pred_proba,\n",
        "        'Optimal Precision': optimal_precision,  # 최적 임계값에서의 정밀도\n",
        "        'Optimal Recall': optimal_recall,        # 최적 임계값에서의 재현율\n",
        "        'Precision Curve': precision_curve,      # 전체 정밀도 곡선\n",
        "        'Recall Curve': recall_curve             # 전체 재현율 곡선\n",
        "    }\n",
        "\n",
        "models = {\n",
        "    'CatBoost': catboost_model,\n",
        "    'LightGBM': lgb_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'RandomForest': rf_model,\n",
        "    'Stacking': stacking_model\n",
        "}\n",
        "# 각 모델 평가\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Evaluating {name} model...\")\n",
        "    results[name] = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# AUC 기준 모델 성능 비교\n",
        "print(\"\\nModel performance comparison (sorted by AUC):\")\n",
        "for name, metrics in sorted(results.items(), key=lambda x: x[1]['AUC'], reverse=True):\n",
        "    print(f\"{name}: Precision = {metrics['Optimal Precision']:.4f}, Recall = {metrics['Optimal Recall']:.4f}, F1 Score = {metrics['F1 Score']:.4f}, AUC = {metrics['AUC']:.4f}, Threshold = {metrics['Optimal Threshold']:.4f}\")\n",
        "\n",
        "# F1 점수 기준 모델 성능 비교\n",
        "print(\"\\nModel performance comparison (sorted by F1 Score):\")\n",
        "for name, metrics in sorted(results.items(), key=lambda x: x[1]['F1 Score'], reverse=True):\n",
        "    print(f\"{name}: F1 Score = {metrics['F1 Score']:.4f}, AUC = {metrics['AUC']:.4f}, Threshold = {metrics['Optimal Threshold']:.4f}\")\n",
        "\n",
        "# AUC 및 F1 점수 시각화\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# AUC 비교\n",
        "plt.subplot(1, 2, 1)\n",
        "auc_values = [metrics['AUC'] for name, metrics in results.items()]\n",
        "model_names = list(results.keys())\n",
        "\n",
        "sns.barplot(x=model_names, y=auc_values)\n",
        "plt.title('AUC Comparison')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# F1 Score 비교\n",
        "plt.subplot(1, 2, 2)\n",
        "f1_values = [metrics['F1 Score'] for name, metrics in results.items()]\n",
        "\n",
        "sns.barplot(x=model_names, y=f1_values)\n",
        "plt.title('F1 Score Comparison')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zGJ0aR67YX7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "hKEadGlyZJKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP"
      ],
      "metadata": {
        "id": "1sbbOiTDrIUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'CatBoost': catboost_model,\n",
        "    'LightGBM': lgb_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'RandomForest': rf_model,\n",
        "    'Stacking': stacking_model\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# SHAP FEATURE IMPORTANCE ANALYSIS\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# feature_names 정의 확인\n",
        "if 'feature_names' not in globals():\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "# SHAP 값 계산 함수\n",
        "def analyze_shap_values(models, X_test, feature_names):\n",
        "    shap_values_dict = {}\n",
        "\n",
        "    # 샘플 데이터 준비 (더 많은 샘플 사용)\n",
        "    n_samples = min(1000, X_test.shape[0])\n",
        "    X_sample = X_test.iloc[:n_samples]\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Calculating SHAP values for {name} model...\")\n",
        "        try:\n",
        "            # 모델 유형에 관계없이 일관된 방식으로 처리\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "            # 이진 분류 모델 처리\n",
        "            if isinstance(shap_values, list):\n",
        "                if len(shap_values) > 1:\n",
        "                    shap_values = shap_values[1]  # 양성 클래스(사망)의 SHAP 값\n",
        "\n",
        "            shap_values_dict[name] = {\n",
        "                'values': shap_values,\n",
        "                'explainer': explainer,\n",
        "                'data': X_sample\n",
        "            }\n",
        "            print(f\"Successfully calculated SHAP values for {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating SHAP values for {name}: {e}\")\n",
        "\n",
        "    return shap_values_dict\n",
        "\n",
        "# SHAP 값 계산\n",
        "shap_values_dict = analyze_shap_values(models, X_test, feature_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "U6z55WdtoM4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP 결과가 있는 경우에만 시각화 진행\n",
        "if shap_values_dict:\n",
        "    # 각 모델에 대한 SHAP summary plot (특성 영향 방향 표시)\n",
        "    for name, shap_data in shap_values_dict.items():\n",
        "        plt.close('all')\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        try:\n",
        "            # 빨간색/파란색으로 특성의 양성/음성 영향을 표시하는 plot\n",
        "            shap.summary_plot(\n",
        "                shap_data['values'],\n",
        "                shap_data['data'],\n",
        "                feature_names=feature_names,\n",
        "                max_display=10,  # 상위 20개 특성 표시\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Feature Impact - {name}', fontsize=16)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'shap_feature_impact_{name}.png', bbox_inches='tight')\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting SHAP summary for {name}: {e}\")\n",
        "\n",
        "        # 상위 특성에 대한 SHAP force plot (샘플별 영향 시각화)\n",
        "        try:\n",
        "            # 몇 개의 샘플에 대한 force plot\n",
        "            plt.figure(figsize=(20, 3))\n",
        "            sample_idx = 0  # 첫 번째 샘플\n",
        "\n",
        "            # SHAP base_value (평균 예측값) 가져오기\n",
        "            exp_value = shap_data['explainer'].expected_value\n",
        "            if isinstance(exp_value, list):\n",
        "                exp_value = exp_value[1]  # 이진 분류의 경우\n",
        "\n",
        "            # Force plot 생성 (특성이 예측에 미치는 영향 표시)\n",
        "            shap.force_plot(\n",
        "                exp_value,\n",
        "                shap_data['values'][sample_idx, :],\n",
        "                shap_data['data'].iloc[sample_idx],\n",
        "                feature_names=feature_names,\n",
        "                matplotlib=True,\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Force Plot - Sample {sample_idx}', fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'shap_force_plot_{name}_sample_{sample_idx}.png', bbox_inches='tight')\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating force plot: {e}\")\n",
        "\n",
        "        # 상위 특성들에 대한 의존성 그래프 (특성값과 SHAP 값의 관계)\n",
        "        # 상위 5개 특성 가져오기\n",
        "        mean_abs_shap = np.abs(shap_data['values']).mean(axis=0)\n",
        "        top_indices = np.argsort(-mean_abs_shap)[:5]\n",
        "\n",
        "        for idx in top_indices:\n",
        "            feature = feature_names[idx]\n",
        "            plt.close('all')\n",
        "            plt.figure(figsize=(10, 7))\n",
        "\n",
        "            try:\n",
        "                # 의존성 그래프 (특성값이 SHAP 값에 미치는 영향)\n",
        "                shap.dependence_plot(\n",
        "                    idx,\n",
        "                    shap_data['values'],\n",
        "                    shap_data['data'],\n",
        "                    feature_names=feature_names,\n",
        "                    interaction_index=None,\n",
        "                    show=False\n",
        "                )\n",
        "                plt.title(f'SHAP Dependence Plot - {feature}', fontsize=14)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f'shap_dependence_{name}_{feature}.png', bbox_inches='tight')\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating dependence plot for {feature}: {e}\")\n",
        "else:\n",
        "    print(\"No SHAP values were calculated. Check for errors in the previous steps.\")"
      ],
      "metadata": {
        "id": "6ptBDG6Up9V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values_dict"
      ],
      "metadata": {
        "id": "qxPounmpoyHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "\n",
        "import pickle\n",
        "\n",
        "models_to_save = {\n",
        "    'catboost': catboost_model,\n",
        "    'lightgbm': lgb_model,\n",
        "    'xgboost': xgb_model,\n",
        "    'randomforest': rf_model,\n",
        "    'stacking': stacking_model\n",
        "}\n",
        "\n",
        "import os\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "\n",
        "for name, model in models_to_save.items():\n",
        "    with open(f'saved_models/{name}_model.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"{name} 모델이 saved_models/{name}_model.pkl에 저장되었습니다.\")\n",
        "\n",
        "# with open('saved_models/stacking_model.pkl', 'rb') as f:\n",
        "#     loaded_stacking_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "nJ2PEvGXfHKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Model"
      ],
      "metadata": {
        "id": "vZimflxOq7tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Function to add mortality prediction results for a specific patient ID\n",
        "def add_mortality_prediction_for_patient(data_path, patient_id, model_path, output_path=None):\n",
        "    \"\"\"\n",
        "    Predicts 90-day mortality for a specified patient ID and adds it to the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_path : str\n",
        "        Path to the original data file\n",
        "    patient_id : int\n",
        "        ICU admission ID of the patient to predict\n",
        "    model_path : str\n",
        "        Path to the mortality prediction model file\n",
        "    output_path : str, optional\n",
        "        Path to save the results. If None, does not save\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (mortality prediction probability, mortality prediction label (0 or 1), updated dataframe)\n",
        "    \"\"\"\n",
        "    # 1. Load data\n",
        "    raw_data = pd.read_csv(data_path)\n",
        "\n",
        "    # 2. Filter data for the specific patient\n",
        "    patient_data = raw_data[raw_data['icustayid'] == patient_id]\n",
        "\n",
        "    if len(patient_data) == 0:\n",
        "        print(f\"Error: Patient ID {patient_id} not found in the dataset\")\n",
        "        return None, None, raw_data\n",
        "\n",
        "    # 3. Load model\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    # 4. Preprocess patient data\n",
        "    aggregated_data = aggregate_patient_data(patient_data)\n",
        "    cleaned_data = clean_feature_names(aggregated_data)\n",
        "\n",
        "    # 5. Prepare features needed for prediction\n",
        "    X = cleaned_data\n",
        "\n",
        "    # 6. Check all features expected by the model\n",
        "    if hasattr(model, 'feature_names_in_'):\n",
        "        required_features = model.feature_names_in_\n",
        "    elif hasattr(model, '_feature_name'):\n",
        "        required_features = model._feature_name  # LightGBM model\n",
        "    else:\n",
        "        required_features = []\n",
        "\n",
        "    # 7. Handle missing features\n",
        "    if len(required_features) > 0:\n",
        "        missing_features = set(required_features) - set(X.columns)\n",
        "        print(f\"Missing {len(missing_features)} features\")\n",
        "\n",
        "        for feature in missing_features:\n",
        "            X[feature] = 0  # Set default value for missing features\n",
        "\n",
        "        # Remove extra features not needed by the model\n",
        "        extra_features = set(X.columns) - set(required_features)\n",
        "        if extra_features:\n",
        "            X = X.drop(columns=list(extra_features))\n",
        "\n",
        "        # Sort features in the exact order\n",
        "        X = X[required_features]\n",
        "\n",
        "    # 8. Predict mortality\n",
        "    try:\n",
        "        mortality_prob = model.predict_proba(X)[0][1]\n",
        "        mortality_label = 1 if mortality_prob >= 0.5 else 0\n",
        "        print(f\"Patient {patient_id}: Mortality prediction = {mortality_label} (Probability: {mortality_prob:.2%})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting for patient {patient_id}: {e}\")\n",
        "        mortality_prob = 0\n",
        "        mortality_label = 0\n",
        "\n",
        "    # 9. Add prediction results to the dataset\n",
        "    # Create mortality_90d column if it doesn't exist\n",
        "    if 'mortality_90d' not in raw_data.columns:\n",
        "        raw_data['mortality_90d'] = 0\n",
        "\n",
        "    # Update only the prediction result for the specific patient ID\n",
        "    raw_data.loc[raw_data['icustayid'] == patient_id, 'mortality_90d'] = mortality_label\n",
        "\n",
        "    # 10. Save results (if requested)\n",
        "    if output_path:\n",
        "        raw_data.to_csv(output_path, index=False)\n",
        "        print(f\"Saved dataset with mortality prediction to {output_path}\")\n",
        "\n",
        "    # 11. Return results\n",
        "    return mortality_prob, mortality_label, raw_data\n",
        "\n",
        "# Function to aggregate patient data\n",
        "def aggregate_patient_data(df):\n",
        "    # Check current columns in the dataframe\n",
        "    columns = df.columns.tolist()\n",
        "\n",
        "    # Create aggregation dictionary\n",
        "    agg_dict = {}\n",
        "\n",
        "    # For numeric columns, calculate mean, max, min, standard deviation\n",
        "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
        "    for col in numeric_columns:\n",
        "        if col != 'icustayid':  # exclude icustayid as it's the grouping key\n",
        "            agg_dict[col] = ['mean', 'max', 'min', 'std']\n",
        "\n",
        "    # Add specific columns only if they exist\n",
        "    if 'mortality_90d' in columns:\n",
        "        agg_dict['mortality_90d'] = 'max'  # mortality is the maximum value (if 1 exists, patient died)\n",
        "\n",
        "    # Chart time is counted (number of records)\n",
        "    if 'charttime' in columns:\n",
        "        agg_dict['charttime'] = 'count'\n",
        "\n",
        "    # Group and aggregate\n",
        "    agg_df = df.groupby('icustayid').agg(agg_dict)\n",
        "\n",
        "    # Clean column names\n",
        "    agg_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in agg_df.columns]\n",
        "\n",
        "    return agg_df\n",
        "\n",
        "# Function to clean feature names\n",
        "def clean_feature_names(df):\n",
        "    clean_df = df.copy()\n",
        "\n",
        "    clean_columns = {}\n",
        "    for col in clean_df.columns:\n",
        "        new_col = re.sub(r'[^\\w\\s_]', '', col)\n",
        "        new_col = re.sub(r'\\s+', '_', new_col)\n",
        "        clean_columns[col] = new_col\n",
        "\n",
        "    clean_df = clean_df.rename(columns=clean_columns)\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Get user input\n",
        "    data_path = '/content/drive/MyDrive/project2/AI_agent_test_sepsis_features.csv'\n",
        "    patient_id = int(input(\"Enter patient ID to predict: \"))\n",
        "    model_path = '/content/saved_models/lightgbm_model.pkl'\n",
        "\n",
        "    # Save output file option\n",
        "    save_output = input(\"Do you want to save the results to a file? (y/n): \").lower() == 'y'\n",
        "    output_path = None\n",
        "    if save_output:\n",
        "        output_path = input(\"Enter the file path to save: \")\n",
        "\n",
        "    # Add mortality prediction to patient data\n",
        "    mortality_prob, mortality_label, updated_data = add_mortality_prediction_for_patient(\n",
        "        data_path, patient_id, model_path, output_path\n",
        "    )\n",
        "\n",
        "    # Print result summary\n",
        "    if mortality_prob is not None:\n",
        "        print(\"\\n========== Prediction Result Summary ==========\")\n",
        "        print(f\"Patient ID: {patient_id}\")\n",
        "        print(f\"Death ratio: {mortality_prob:.2%}\")\n",
        "        print(f\"Death prediction: {'Yes' if mortality_label == 1 else 'No'}\")\n",
        "        print(\"==============================================\")"
      ],
      "metadata": {
        "id": "SfRY_dMtbiVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# import json\n",
        "# import google.generativeai as genai  # Gemini API 사용\n",
        "\n",
        "# # SHAP 값 및 중요 특성 추출 함수\n",
        "# def extract_patient_shap_data(patient_id, shap_values_dict, feature_names, X_data):\n",
        "#     \"\"\"\n",
        "#     특정 환자의 SHAP 값을 추출하는 함수\n",
        "#     \"\"\"\n",
        "#     # 주요 특성 목록 (SHAP 그래프에서 확인된 상위 10개)\n",
        "#     key_features = [\n",
        "#         'age_first', 'BUN_last', 'output_4hourly_std', 'SOFA_mean',\n",
        "#         'Weight_kg_mean', 'input_4hourly_last', 'mechvent_std',\n",
        "#         'GCS_mean', 'SIRS_mean', 'Creatinine_min'\n",
        "#     ]\n",
        "\n",
        "#     # 환자 데이터 찾기\n",
        "#     patient_data = X_data[X_data['icustayid'] == patient_id]\n",
        "#     if len(patient_data) == 0:\n",
        "#         return None\n",
        "\n",
        "#     # 모델 이름 가져오기\n",
        "#     model_name = list(shap_values_dict.keys())[0]\n",
        "#     shap_data = shap_values_dict[model_name]\n",
        "\n",
        "#     # SHAP 데이터에서 환자 인덱스 찾기\n",
        "#     X_sample = shap_data['data']\n",
        "#     if 'icustayid' in X_sample.columns:\n",
        "#         patient_idx = X_sample[X_sample['icustayid'] == patient_id].index\n",
        "#         if len(patient_idx) == 0:\n",
        "#             return None\n",
        "#         patient_idx = patient_idx[0]\n",
        "#     else:\n",
        "#         # 환자 ID가 없는 경우, 샘플 데이터의 첫 번째 행 사용 (데모 목적)\n",
        "#         patient_idx = 0\n",
        "\n",
        "#     # SHAP 값 추출\n",
        "#     shap_values = shap_data['values']\n",
        "\n",
        "#     # 환자 특성 및 SHAP 값 저장\n",
        "#     patient_info = {}\n",
        "\n",
        "#     for feature in key_features:\n",
        "#         if feature in feature_names:\n",
        "#             feature_idx = feature_names.index(feature)\n",
        "\n",
        "#             # 특성값\n",
        "#             feature_value = X_sample.iloc[patient_idx][feature]\n",
        "\n",
        "#             # SHAP 값\n",
        "#             shap_value = shap_values[patient_idx, feature_idx]\n",
        "\n",
        "#             # 결과 저장\n",
        "#             patient_info[feature] = {\n",
        "#                 'value': float(feature_value),\n",
        "#                 'shap': float(shap_value),\n",
        "#                 'impact': 'positive' if shap_value > 0 else 'negative'\n",
        "#             }\n",
        "\n",
        "#     # 전체 SHAP 값 합계 및 base_value\n",
        "#     base_value = shap_data['explainer'].expected_value\n",
        "#     if isinstance(base_value, list):\n",
        "#         base_value = base_value[1]  # 이진 분류의 경우\n",
        "\n",
        "#     patient_info['base_value'] = float(base_value)\n",
        "#     patient_info['total_shap'] = float(np.sum(shap_values[patient_idx]))\n",
        "\n",
        "#     return patient_info\n",
        "\n",
        "# # Gemini API를 사용한 설명 생성 함수\n",
        "# def explain_mortality_with_gemini(patient_id, mortality_prob, patient_shap_data, api_key):\n",
        "#     \"\"\"\n",
        "#     Gemini API를 사용하여 환자의 사망률을 설명하는 함수\n",
        "#     \"\"\"\n",
        "#     # Gemini API 설정\n",
        "#     genai.configure(api_key=api_key)\n",
        "\n",
        "#     # 특성에 대한 설명 정보\n",
        "#     feature_descriptions = {\n",
        "#         'age_first': '환자의 나이 (첫 측정)',\n",
        "#         'BUN_last': '혈액요소질소(BUN) 수치 (마지막 측정)',\n",
        "#         'output_4hourly_std': '4시간 간격 소변량의 표준편차',\n",
        "#         'SOFA_mean': '순차적 장기부전 평가(SOFA) 점수의 평균값',\n",
        "#         'Weight_kg_mean': '환자 체중(kg)의 평균값',\n",
        "#         'input_4hourly_last': '4시간 간격 수액 주입량 (마지막 측정)',\n",
        "#         'mechvent_std': '기계 환기 사용의 표준편차',\n",
        "#         'GCS_mean': '글래스고 혼수 척도(GCS) 점수의 평균값',\n",
        "#         'SIRS_mean': '전신성 염증 반응 증후군(SIRS) 점수의 평균값',\n",
        "#         'Creatinine_min': '크레아티닌 수치의 최소값'\n",
        "#     }\n",
        "\n",
        "#     # 프롬프트 생성\n",
        "#     prompt = f\"\"\"\n",
        "#     당신은 중환자실 환자의 의료 데이터를 분석하여 90일 내 사망 가능성을 예측하고 설명하는 AI 의료 조수입니다.\n",
        "\n",
        "#     환자 ID {patient_id}의 사망 확률은 {mortality_prob:.1f}%입니다.\n",
        "\n",
        "#     이 예측은 SHAP(SHapley Additive exPlanations) 값을 기반으로 한 머신러닝 모델에서 나왔습니다.\n",
        "#     SHAP 값은 각 특성(환자의 생체 지표, 임상 데이터 등)이 사망 위험을 높이는지(양수) 또는 낮추는지(음수)를 나타냅니다.\n",
        "\n",
        "#     아래는 이 환자의 주요 특성 값과 그에 따른 SHAP 값입니다:\n",
        "\n",
        "#     {json.dumps(patient_shap_data, indent=2)}\n",
        "\n",
        "#     특성 설명:\n",
        "#     {json.dumps(feature_descriptions, indent=2)}\n",
        "\n",
        "#     위 데이터를 바탕으로 다음 질문에 답해주세요:\n",
        "#     1. 이 환자의 사망 위험을 증가시키는 가장 중요한 요인 3가지는 무엇인가요?\n",
        "#     2. 이 환자의 사망 위험을 감소시키는 가장 중요한 요인 3가지는 무엇인가요?\n",
        "#     3. 각 요인이 왜 그런 영향을 미치는지 의학적으로 설명해주세요.\n",
        "#     4. 이 환자의 종합적인 건강 상태와 위험 요인을 요약해주세요.\n",
        "#     5. 이 환자의 생존 가능성을 높이기 위해 어떤 중재나 치료가 효과적일 수 있을지 제안해주세요.\n",
        "\n",
        "#     의사와 간호사가 이해할 수 있는 전문적인 답변을 제공하되, 환자의 가족도 이해할 수 있도록 명확하게 설명해주세요.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Gemini 모델 설정 및 예측\n",
        "#     model = genai.GenerativeModel('gemini-pro')\n",
        "#     response = model.generate_content(prompt)\n",
        "\n",
        "#     return response.text\n",
        "\n",
        "# # 실제 사용 예시\n",
        "# def mortality_explanation_agent(patient_id, X_data, model, shap_values_dict, feature_names, gemini_api_key):\n",
        "#     \"\"\"\n",
        "#     환자 ID를 입력받아 사망률 예측과 설명을 제공하는 에이전트\n",
        "#     \"\"\"\n",
        "#     # 환자 데이터 찾기\n",
        "#     patient_data = X_data[X_data['icustayid'] == patient_id]\n",
        "#     if len(patient_data) == 0:\n",
        "#         return f\"환자 ID {patient_id}를 찾을 수 없습니다.\"\n",
        "\n",
        "#     # 사망률 예측\n",
        "#     try:\n",
        "#         mortality_prob = model.predict_proba(patient_data)[0][1] * 100\n",
        "#     except Exception as e:\n",
        "#         return f\"사망률 예측 중 오류가 발생했습니다: {e}\"\n",
        "\n",
        "#     # SHAP 값 추출\n",
        "#     patient_shap_data = extract_patient_shap_data(patient_id, shap_values_dict, feature_names, X_data)\n",
        "#     if patient_shap_data is None:\n",
        "#         return f\"환자 ID {patient_id}의 SHAP 데이터를 찾을 수 없습니다.\"\n",
        "\n",
        "#     # Gemini를 이용한 설명 생성\n",
        "#     explanation = explain_mortality_with_gemini(patient_id, mortality_prob, patient_shap_data, gemini_api_key)\n",
        "\n",
        "#     return explanation\n",
        "\n",
        "\n",
        "# gemini_api_key = \"AIzaSyAiio5qPFqngMcqic8pSZNbSJNOhf1eaBE\"  # 실제 API 키로 대체\n",
        "# patient_explanation = mortality_explanation_agent(\n",
        "#     patient_id=200003,\n",
        "#     X_data='/content/drive/MyDrive/project2/AI_agent_train_sepsis.csv',\n",
        "#     model='/content/saved_models/lightgbm_model_weight4_leav100.pkl',\n",
        "#     shap_values_dict=shap_values_dict,\n",
        "#     feature_names=feature_names,\n",
        "#     gemini_api_key=gemini_api_key\n",
        "# )\n",
        "# print(patient_explanation)"
      ],
      "metadata": {
        "id": "HdSrCieCq5wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# import json\n",
        "# import re\n",
        "# import matplotlib.pyplot as plt\n",
        "# import shap\n",
        "# import google.generativeai as genai\n",
        "\n",
        "# # 1. 데이터 및 모델 로드 함수\n",
        "# def load_data_and_model(data_path, model_path):\n",
        "#     \"\"\"\n",
        "#     데이터와 모델을 파일에서 로드하는 함수\n",
        "#     \"\"\"\n",
        "#     # 데이터 로드\n",
        "#     print(f\"Loading data from {data_path}...\")\n",
        "#     data = pd.read_csv(data_path)\n",
        "\n",
        "#     # 모델 로드\n",
        "#     print(f\"Loading model from {model_path}...\")\n",
        "#     with open(model_path, 'rb') as f:\n",
        "#         model = pickle.load(f)\n",
        "\n",
        "#     return data, model\n",
        "\n",
        "# # 2. 특정 환자에 대한 사망률 예측 함수\n",
        "# def predict_mortality_for_patient(patient_id, data, model):\n",
        "#     \"\"\"\n",
        "#     특정 환자에 대한 사망률 예측 함수\n",
        "#     \"\"\"\n",
        "#     # 환자 데이터 필터링\n",
        "#     patient_data = data[data['icustayid'] == patient_id]\n",
        "\n",
        "#     if len(patient_data) == 0:\n",
        "#         print(f\"Error: Patient ID {patient_id} not found in the dataset\")\n",
        "#         return None, None\n",
        "\n",
        "#     # 데이터 전처리\n",
        "#     aggregated_data = aggregate_patient_data(patient_data)\n",
        "#     cleaned_data = clean_feature_names(aggregated_data)\n",
        "\n",
        "#     # 예측에 필요한 특성 준비\n",
        "#     X = cleaned_data\n",
        "\n",
        "#     # 모델이 기대하는 모든 특성 확인\n",
        "#     if hasattr(model, 'feature_names_in_'):\n",
        "#         required_features = model.feature_names_in_\n",
        "#     elif hasattr(model, '_feature_name'):\n",
        "#         required_features = model._feature_name  # LightGBM 모델\n",
        "#     else:\n",
        "#         required_features = []\n",
        "\n",
        "#     # 누락된 특성 처리\n",
        "#     if len(required_features) > 0:\n",
        "#         missing_features = set(required_features) - set(X.columns)\n",
        "#         print(f\"Missing {len(missing_features)} features\")\n",
        "\n",
        "#         for feature in missing_features:\n",
        "#             X[feature] = 0  # 누락된 특성에 기본값 설정\n",
        "\n",
        "#         # 모델에 필요하지 않은 추가 특성 제거\n",
        "#         extra_features = set(X.columns) - set(required_features)\n",
        "#         if extra_features:\n",
        "#             X = X.drop(columns=list(extra_features))\n",
        "\n",
        "#         # 정확한 순서로 특성 정렬\n",
        "#         X = X[required_features]\n",
        "\n",
        "#     # 사망률 예측\n",
        "#     try:\n",
        "#         mortality_prob = model.predict_proba(X)[0][1]\n",
        "#         mortality_label = 1 if mortality_prob >= 0.5 else 0\n",
        "#         print(f\"Patient {patient_id}: Mortality prediction = {mortality_label} (Probability: {mortality_prob:.2%})\")\n",
        "#         return mortality_prob, mortality_label\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error predicting for patient {patient_id}: {e}\")\n",
        "#         return None, None\n",
        "\n",
        "# # 3. 환자 데이터 집계 함수\n",
        "# def aggregate_patient_data(df):\n",
        "#     # 데이터프레임의 현재 컬럼 목록 확인\n",
        "#     columns = df.columns.tolist()\n",
        "\n",
        "#     # 집계 딕셔너리 생성\n",
        "#     agg_dict = {}\n",
        "\n",
        "#     # 숫자형 컬럼에 대해서는 평균, 최대, 최소, 표준편차 계산\n",
        "#     numeric_columns = df.select_dtypes(include=['number']).columns\n",
        "#     for col in numeric_columns:\n",
        "#         if col != 'icustayid':  # icustayid는 그룹화 키이므로 제외\n",
        "#             agg_dict[col] = ['mean', 'max', 'min', 'std']\n",
        "\n",
        "#     # 특정 컬럼이 있는 경우에만 추가\n",
        "#     if 'mortality_90d' in columns:\n",
        "#         agg_dict['mortality_90d'] = 'max'  # 사망 여부는 최대값(1이 있으면 사망)\n",
        "\n",
        "#     # 차트 시간은 카운트(레코드 수)\n",
        "#     if 'charttime' in columns:\n",
        "#         agg_dict['charttime'] = 'count'\n",
        "\n",
        "#     # 그룹화 및 집계\n",
        "#     agg_df = df.groupby('icustayid').agg(agg_dict)\n",
        "\n",
        "#     # 컬럼 이름 정리\n",
        "#     agg_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in agg_df.columns]\n",
        "\n",
        "#     return agg_df\n",
        "\n",
        "# # 4. 특성 이름 정리 함수\n",
        "# def clean_feature_names(df):\n",
        "#     clean_df = df.copy()\n",
        "\n",
        "#     clean_columns = {}\n",
        "#     for col in clean_df.columns:\n",
        "#         new_col = re.sub(r'[^\\w\\s_]', '', col)\n",
        "#         new_col = re.sub(r'\\s+', '_', new_col)\n",
        "#         clean_columns[col] = new_col\n",
        "\n",
        "#     clean_df = clean_df.rename(columns=clean_columns)\n",
        "\n",
        "#     return clean_df\n",
        "\n",
        "# # 5. SHAP 값 계산 함수 (디버깅 로그 추가)\n",
        "# def analyze_shap_values(model, X_data, feature_names):\n",
        "#     \"\"\"\n",
        "#     SHAP 값을 계산하는 함수 (디버깅 로그 추가)\n",
        "#     \"\"\"\n",
        "#     # 모델 딕셔너리화 (분석 코드와 호환성을 위해)\n",
        "#     models = {'LightGBM': model}\n",
        "#     shap_values_dict = {}\n",
        "\n",
        "#     # 샘플 데이터 준비\n",
        "#     print(f\"Preparing sample data from {len(X_data)} rows...\")\n",
        "#     n_samples = min(1000, X_data.shape[0])\n",
        "#     X_sample = X_data.iloc[:n_samples]\n",
        "\n",
        "#     for name, model in models.items():\n",
        "#         print(f\"Calculating SHAP values for {name} model...\")\n",
        "#         try:\n",
        "#             # SHAP TreeExplainer에 대한 옵션 추가\n",
        "#             print(\"Creating SHAP TreeExplainer...\")\n",
        "#             explainer = shap.TreeExplainer(model)\n",
        "\n",
        "#             print(f\"Calculating SHAP values for {n_samples} samples...\")\n",
        "\n",
        "#             # SHAP 값 계산 중 오류 방지 옵션 추가\n",
        "#             old_options = np.get_printoptions()\n",
        "#             np.set_printoptions(threshold=10000)\n",
        "#             try:\n",
        "#                 # predict_disable_shape_check=True 옵션 추가\n",
        "#                 if hasattr(model, 'set_params'):\n",
        "#                     model.set_params(predict_disable_shape_check=True)\n",
        "#                 shap_values = explainer.shap_values(X_sample)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"First attempt error: {e}\")\n",
        "#                 print(\"Trying with fewer samples...\")\n",
        "#                 # 샘플 수 줄이기\n",
        "#                 X_sample = X_data.iloc[:100]\n",
        "#                 shap_values = explainer.shap_values(X_sample)\n",
        "#             finally:\n",
        "#                 np.set_printoptions(**old_options)\n",
        "\n",
        "#             # 이진 분류 모델 처리\n",
        "#             print(\"Processing SHAP values...\")\n",
        "#             if isinstance(shap_values, list):\n",
        "#                 print(f\"SHAP values is a list with {len(shap_values)} elements\")\n",
        "#                 if len(shap_values) > 1:\n",
        "#                     print(f\"Using element 1 with shape {shap_values[1].shape}\")\n",
        "#                     shap_values = shap_values[1]  # 양성 클래스(사망)의 SHAP 값\n",
        "#                 else:\n",
        "#                     print(f\"Using element 0 with shape {shap_values[0].shape}\")\n",
        "#                     shap_values = shap_values[0]\n",
        "\n",
        "#             shap_values_dict[name] = {\n",
        "#                 'values': shap_values,\n",
        "#                 'explainer': explainer,\n",
        "#                 'data': X_sample\n",
        "#             }\n",
        "#             print(f\"Successfully calculated SHAP values for {name}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error calculating SHAP values for {name}: {e}\")\n",
        "#             print(\"Stack trace:\", e.__traceback__)\n",
        "\n",
        "#     # 결과 확인\n",
        "#     if not shap_values_dict:\n",
        "#         print(\"WARNING: No SHAP values were calculated successfully\")\n",
        "#         # 더미 데이터 생성\n",
        "#         print(\"Creating dummy SHAP data for demonstration purposes\")\n",
        "#         X_sample = X_data.iloc[:10]\n",
        "#         dummy_values = np.zeros((len(X_sample), len(feature_names)))\n",
        "\n",
        "#         # 더미 explainer 생성\n",
        "#         class DummyExplainer:\n",
        "#             def __init__(self):\n",
        "#                 self.expected_value = 0.0\n",
        "\n",
        "#         shap_values_dict['LightGBM'] = {\n",
        "#             'values': dummy_values,\n",
        "#             'explainer': DummyExplainer(),\n",
        "#             'data': X_sample\n",
        "#         }\n",
        "#         print(\"Created dummy SHAP data\")\n",
        "\n",
        "#     return shap_values_dict, feature_names\n",
        "\n",
        "# # 6. 특정 환자의 SHAP 값 추출 함수 (예외 처리 강화)\n",
        "# def extract_patient_shap_data(patient_id, X_data, shap_values_dict, feature_names):\n",
        "#     \"\"\"\n",
        "#     특정 환자의 SHAP 값을 추출하는 함수 (예외 처리 강화)\n",
        "#     \"\"\"\n",
        "#     # shap_values_dict가 비어 있는지 확인\n",
        "#     if not shap_values_dict:\n",
        "#         print(\"Error: SHAP values dictionary is empty\")\n",
        "#         return dummy_shap_data(patient_id, feature_names)\n",
        "\n",
        "#     # 주요 특성 목록 (SHAP 그래프에서 확인된 상위 10개)\n",
        "#     key_features = [\n",
        "#         'age_first', 'BUN_last', 'output_4hourly_std', 'SOFA_mean',\n",
        "#         'Weight_kg_mean', 'input_4hourly_last', 'mechvent_std',\n",
        "#         'GCS_mean', 'SIRS_mean', 'Creatinine_min'\n",
        "#     ]\n",
        "\n",
        "#     try:\n",
        "#         # 모델 이름 가져오기\n",
        "#         model_names = list(shap_values_dict.keys())\n",
        "#         if not model_names:\n",
        "#             print(\"Error: No models found in SHAP values dictionary\")\n",
        "#             return dummy_shap_data(patient_id, feature_names)\n",
        "\n",
        "#         model_name = model_names[0]\n",
        "#         shap_data = shap_values_dict[model_name]\n",
        "\n",
        "#         # SHAP 데이터에서 환자 인덱스 찾기\n",
        "#         X_sample = shap_data['data']\n",
        "\n",
        "#         # X_sample에 icustayid 열이 있는지 확인\n",
        "#         if 'icustayid' in X_sample.columns:\n",
        "#             patient_idx = X_sample[X_sample['icustayid'] == patient_id].index\n",
        "#             if len(patient_idx) == 0:\n",
        "#                 print(f\"Warning: Patient ID {patient_id} not found in SHAP sample data\")\n",
        "#                 # 대체 방법: 원본 데이터에서 환자 찾기\n",
        "#                 orig_idx = X_data[X_data['icustayid'] == patient_id].index\n",
        "#                 if len(orig_idx) == 0:\n",
        "#                     print(f\"Error: Patient ID {patient_id} not found in any dataset\")\n",
        "#                     return dummy_shap_data(patient_id, feature_names)\n",
        "#                 # 샘플에 포함되지 않았으므로 첫 번째 샘플 사용 (데모 목적)\n",
        "#                 patient_idx = 0\n",
        "#             else:\n",
        "#                 patient_idx = patient_idx[0]\n",
        "#         else:\n",
        "#             # icustayid 열이 없는 경우, 첫 번째 샘플 사용 (데모 목적)\n",
        "#             print(\"Warning: icustayid column not found in SHAP sample data, using first sample\")\n",
        "#             patient_idx = 0\n",
        "\n",
        "#         # SHAP 값 추출\n",
        "#         shap_values = shap_data['values']\n",
        "\n",
        "#         # 환자 특성 및 SHAP 값 저장\n",
        "#         patient_info = {}\n",
        "\n",
        "#         for feature in key_features:\n",
        "#             if feature in feature_names:\n",
        "#                 try:\n",
        "#                     feature_idx = feature_names.index(feature)\n",
        "\n",
        "#                     # 특성값\n",
        "#                     feature_value = X_sample.iloc[patient_idx][feature]\n",
        "\n",
        "#                     # SHAP 값\n",
        "#                     shap_value = shap_values[patient_idx, feature_idx]\n",
        "\n",
        "#                     # 결과 저장\n",
        "#                     patient_info[feature] = {\n",
        "#                         'value': float(feature_value),\n",
        "#                         'shap': float(shap_value),\n",
        "#                         'impact': 'positive' if shap_value > 0 else 'negative'\n",
        "#                     }\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error extracting SHAP data for feature {feature}: {e}\")\n",
        "#                     # 누락된 특성에 대한 더미 데이터 추가\n",
        "#                     patient_info[feature] = {\n",
        "#                         'value': 0.0,\n",
        "#                         'shap': 0.0,\n",
        "#                         'impact': 'neutral'\n",
        "#                     }\n",
        "\n",
        "#         # 전체 SHAP 값 합계 및 base_value\n",
        "#         try:\n",
        "#             base_value = shap_data['explainer'].expected_value\n",
        "#             if isinstance(base_value, list):\n",
        "#                 base_value = base_value[1]  # 이진 분류의 경우\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error extracting base_value: {e}\")\n",
        "#             base_value = 0.0\n",
        "\n",
        "#         try:\n",
        "#             total_shap = np.sum(shap_values[patient_idx])\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error calculating total SHAP: {e}\")\n",
        "#             total_shap = 0.0\n",
        "\n",
        "#         patient_info['base_value'] = float(base_value)\n",
        "#         patient_info['total_shap'] = float(total_shap)\n",
        "\n",
        "#         return patient_info\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error extracting patient SHAP data: {e}\")\n",
        "#         return dummy_shap_data(patient_id, feature_names)\n",
        "\n",
        "# # # 더미 SHAP 데이터 생성 함수\n",
        "# # def dummy_shap_data(patient_id, feature_names):\n",
        "# #     \"\"\"\n",
        "# #     더미 SHAP 데이터를 생성하는 함수 (오류 발생 시 대체용)\n",
        "# #     \"\"\"\n",
        "# #     print(f\"Generating dummy SHAP data for patient {patient_id}\")\n",
        "\n",
        "# #     key_features = [\n",
        "# #         'age_first', 'BUN_last', 'output_4hourly_std', 'SOFA_mean',\n",
        "# #         'Weight_kg_mean', 'input_4hourly_last', 'mechvent_std',\n",
        "# #         'GCS_mean', 'SIRS_mean', 'Creatinine_min'\n",
        "# #     ]\n",
        "\n",
        "# #     patient_info = {}\n",
        "\n",
        "# #     # 모든 특성에 대해 임의의 값 생성\n",
        "# #     for feature in key_features:\n",
        "# #         if feature in feature_names:\n",
        "# #             # 임의의 특성값과 SHAP 값 생성\n",
        "# #             feature_value = np.random.normal(0, 1)\n",
        "# #             shap_value = np.random.normal(0, 0.5)\n",
        "\n",
        "# #             # 결과 저장\n",
        "# #             patient_info[feature] = {\n",
        "# #                 'value': float(feature_value),\n",
        "# #                 'shap': float(shap_value),\n",
        "# #                 'impact': 'positive' if shap_value > 0 else 'negative'\n",
        "# #             }\n",
        "\n",
        "# #     # 기본값 및 총 SHAP 값 설정\n",
        "# #     patient_info['base_value'] = 0.5\n",
        "# #     patient_info['total_shap'] = 0.0\n",
        "\n",
        "# #     return patient_info\n",
        "\n",
        "# # 8. Gemini API를 사용한 설명 생성 함수\n",
        "# def explain_mortality_with_gemini(patient_id, mortality_prob, patient_shap_data, api_key):\n",
        "#     \"\"\"\n",
        "#     Gemini API를 사용하여 환자의 사망률을 설명하는 함수\n",
        "#     \"\"\"\n",
        "#     # Gemini API 설정\n",
        "#     genai.configure(api_key=api_key)\n",
        "\n",
        "#     # 특성에 대한 설명 정보\n",
        "#     feature_descriptions = {\n",
        "#         'age_first': 'Patient\\'s age (first measurement)',\n",
        "#         'BUN_last': 'Blood Urea Nitrogen (BUN) level (last measurement)',\n",
        "#         'output_4hourly_std': 'Standard deviation of urine output measured every 4 hours',\n",
        "#         'SOFA_mean': 'Mean value of Sequential Organ Failure Assessment (SOFA) score',\n",
        "#         'Weight_kg_mean': 'Mean value of patient\\'s weight (kg)',\n",
        "#         'input_4hourly_last': 'Fluid intake measured every 4 hours (last measurement)',\n",
        "#         'mechvent_std': 'Standard deviation of mechanical ventilation usage',\n",
        "#         'GCS_mean': 'Mean value of Glasgow Coma Scale (GCS) score',\n",
        "#         'SIRS_mean': 'Mean value of Systemic Inflammatory Response Syndrome (SIRS) score',\n",
        "#         'Creatinine_min': 'Minimum value of creatinine level',\n",
        "#     }\n",
        "\n",
        "#     # 프롬프트 생성\n",
        "#     prompt = f\"\"\"\n",
        "#     You are an AI medical assistant that analyzes ICU patient medical data to predict and explain the probability of death within 90 days.\n",
        "#     Patient ID {patient_id} has a mortality probability of {mortality_prob:.1f}%.\n",
        "#     This prediction comes from a machine learning model based on SHAP (SHapley Additive exPlanations) values.\n",
        "#     SHAP values indicate whether each feature (patient's vital signs, clinical data, etc.) increases (positive) or decreases (negative) the risk of death.\n",
        "#     Below are the main feature values for this patient and their corresponding SHAP values:\n",
        "#     {json.dumps(patient_shap_data, indent=2)}\n",
        "#     Feature descriptions:\n",
        "#     {json.dumps(feature_descriptions, indent=2)}\n",
        "#     Based on the above data, please answer the following questions:\n",
        "\n",
        "#     What are the 3 most important factors increasing this patient's risk of death?\n",
        "#     What are the 3 most important factors decreasing this patient's risk of death?\n",
        "#     Explain medically why each factor has such an impact.\n",
        "#     Summarize this patient's overall health status and risk factors.\n",
        "#     Suggest what interventions or treatments might be effective in improving this patient's chances of survival.\n",
        "\n",
        "#     Please provide a professional response that doctors and nurses can understand, while also explaining clearly so that the patient's family can understand\n",
        "#     \"\"\"\n",
        "\n",
        "#     try:\n",
        "#         # Gemini 모델 설정 및 예측\n",
        "#         model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "#         response = model.generate_content(prompt)\n",
        "#         return response.text\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error generating explanation with Gemini: {e}\")\n",
        "#         return f\"\"\"\n",
        "#         환자 ID {patient_id}의 사망 확률은 {mortality_prob:.1f}%로 예측됩니다.\n",
        "\n",
        "#         Gemini API에서 오류가 발생하여 자세한 설명을 생성할 수 없습니다.\n",
        "#         오류 메시지: {str(e)}\n",
        "\n",
        "#         주요 특성과 SHAP 값을 참고하여 의료진과 상담하세요.\n",
        "#         \"\"\"\n",
        "\n",
        "# # 9. 종합 실행 함수\n",
        "# def mortality_explanation_agent(patient_id, data_path, model_path, gemini_api_key, visualize=False):\n",
        "#     \"\"\"\n",
        "#     환자 ID를 입력받아 사망률 예측과 설명을 제공하는 종합 에이전트\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # 1. 데이터 및 모델 로드\n",
        "#         X_data, model = load_data_and_model(data_path, model_path)\n",
        "\n",
        "#         # 2. 특성명 목록 생성\n",
        "#         feature_names = X_data.columns.tolist()\n",
        "\n",
        "#         # 3. 사망률 예측\n",
        "#         mortality_prob, mortality_label = predict_mortality_for_patient(patient_id, X_data, model)\n",
        "#         if mortality_prob is None:\n",
        "#             return f\"환자 ID {patient_id}의 사망률을 예측할 수 없습니다.\"\n",
        "\n",
        "#         # 4. SHAP 값 계산\n",
        "#         print(\"Calculating SHAP values...\")\n",
        "#         shap_values_dict, feature_names = analyze_shap_values(model, X_data, feature_names)\n",
        "\n",
        "#         # 5. 환자별 SHAP 값 추출\n",
        "#         patient_shap_data = extract_patient_shap_data(patient_id, X_data, shap_values_dict, feature_names)\n",
        "\n",
        "#         # 7. Gemini를 이용한 설명 생성\n",
        "#         explanation = explain_mortality_with_gemini(patient_id, mortality_prob * 100, patient_shap_data, gemini_api_key)\n",
        "\n",
        "#         return explanation\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Critical error in mortality_explanation_agent: {e}\")\n",
        "#         return f\"\"\"\n",
        "#         환자 ID {patient_id}에 대한 사망률 예측 과정에서 오류가 발생했습니다.\n",
        "#         오류 메시지: {str(e)}\n",
        "\n",
        "#         오류가 반복될 경우 시스템 관리자에게 문의하세요.\n",
        "#         \"\"\"\n",
        "\n",
        "# # 메인 실행 코드\n",
        "# if __name__ == \"__main__\":\n",
        "#     # 설정값\n",
        "#     data_path = '/content/drive/MyDrive/project2/AI_agent_train_sepsis.csv'\n",
        "#     model_path = '/content/saved_models/lightgbm_model_weight4_leav100.pkl'\n",
        "#     gemini_api_key = 'AIzaSyDn1F2k4894S4bvT_SsPnSbH6cjt9Bu06U' # 실제 API 키로 교체\n",
        "\n",
        "#     # 환자 ID 입력 받기\n",
        "#     patient_id = int(input(\"Enter patient ID to analyze: \"))\n",
        "\n",
        "#     # 설명 생성\n",
        "#     explanation = mortality_explanation_agent(\n",
        "#         patient_id=patient_id,\n",
        "#         data_path=data_path,\n",
        "#         model_path=model_path,\n",
        "#         gemini_api_key=gemini_api_key\n",
        "#     )\n",
        "\n",
        "#     # 결과 출력\n",
        "#     print(\"\\n========== Mortality Explanation ==========\")\n",
        "#     print(explanation)\n",
        "#     print(\"===========================================\")"
      ],
      "metadata": {
        "id": "v7WRUSabuLin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# import json\n",
        "# import re\n",
        "# import matplotlib.pyplot as plt\n",
        "# import shap\n",
        "# import google.generativeai as genai\n",
        "\n",
        "# # 1. 데이터 및 모델 로드 함수\n",
        "# def load_data_and_model(data_path, model_path):\n",
        "#     \"\"\"\n",
        "#     데이터와 모델을 파일에서 로드하는 함수\n",
        "#     \"\"\"\n",
        "#     # 데이터 로드\n",
        "#     print(f\"Loading data from {data_path}...\")\n",
        "#     data = pd.read_csv(data_path)\n",
        "\n",
        "#     # 모델 로드\n",
        "#     print(f\"Loading model from {model_path}...\")\n",
        "#     with open(model_path, 'rb') as f:\n",
        "#         model = pickle.load(f)\n",
        "\n",
        "#     return data, model\n",
        "\n",
        "# # 2. 특정 환자에 대한 사망률 예측 함수\n",
        "# def predict_mortality_for_patient(patient_id, data, model):\n",
        "#     \"\"\"\n",
        "#     특정 환자에 대한 사망률 예측 함수 (원래 학습 파이프라인과 일치)\n",
        "#     \"\"\"\n",
        "#     # 환자 데이터 필터링\n",
        "#     patient_data = data[data['icustayid'] == patient_id]\n",
        "\n",
        "#     if len(patient_data) == 0:\n",
        "#         print(f\"Error: Patient ID {patient_id} not found in the dataset\")\n",
        "#         return None, None\n",
        "\n",
        "#     # ====== 원래 학습 파이프라인과 동일한 전처리 적용 ======\n",
        "\n",
        "#     # 1. aggregate_patient_data 함수 정확히 재현\n",
        "#     numerical_cols = [col for col in patient_data.columns if col not in ['mortality_90d', 'icustayid', 'charttime', 'bloc']]\n",
        "#     agg_dict = {}\n",
        "\n",
        "#     # mortality_90d 열이 있으면 추가\n",
        "#     if 'mortality_90d' in patient_data.columns:\n",
        "#         agg_dict['mortality_90d'] = 'first'\n",
        "\n",
        "#     # 수치형 컬럼에 대한 집계 설정\n",
        "#     for col in numerical_cols:\n",
        "#         if col == 'age':\n",
        "#             agg_dict[col] = 'first'\n",
        "#         else:\n",
        "#             agg_dict[col] = ['mean', 'min', 'max', 'std', 'last']\n",
        "\n",
        "#     # charttime 컬럼이 있으면 count 집계 추가\n",
        "#     if 'charttime' in patient_data.columns:\n",
        "#         agg_dict['charttime'] = 'count'\n",
        "\n",
        "#     # 그룹화 및 집계\n",
        "#     aggregated_data = patient_data.groupby('icustayid').agg(agg_dict)\n",
        "\n",
        "#     # 컬럼 이름 정리\n",
        "#     aggregated_data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in aggregated_data.columns]\n",
        "\n",
        "#     # 2. clean_feature_names 함수 적용\n",
        "#     clean_df = aggregated_data.copy()\n",
        "#     clean_columns = {}\n",
        "#     for col in clean_df.columns:\n",
        "#         new_col = re.sub(r'[^\\w\\s_]', '', col)\n",
        "#         new_col = re.sub(r'\\s+', '_', new_col)\n",
        "#         clean_columns[col] = new_col\n",
        "\n",
        "#     cleaned_data = clean_df.rename(columns=clean_columns)\n",
        "\n",
        "#     # 3. 예측에 필요한 특성 준비 (target 변수 제외)\n",
        "#     if 'mortality_90d_first' in cleaned_data.columns:\n",
        "#         X = cleaned_data.drop('mortality_90d_first', axis=1)\n",
        "#     else:\n",
        "#         X = cleaned_data\n",
        "\n",
        "#     # 모델이 기대하는 모든 특성 확인\n",
        "#     if hasattr(model, 'feature_names_in_'):\n",
        "#         required_features = model.feature_names_in_\n",
        "#     elif hasattr(model, '_feature_name'):\n",
        "#         required_features = model._feature_name  # LightGBM 모델\n",
        "#     else:\n",
        "#         required_features = []\n",
        "\n",
        "#     # 누락된 특성 처리\n",
        "#     if len(required_features) > 0:\n",
        "#         missing_features = set(required_features) - set(X.columns)\n",
        "#         print(f\"Missing {len(missing_features)} features\")\n",
        "\n",
        "#         for feature in missing_features:\n",
        "#             X[feature] = 0  # 누락된 특성에 기본값 설정\n",
        "\n",
        "#         # 모델에 필요하지 않은 추가 특성 제거\n",
        "#         extra_features = set(X.columns) - set(required_features)\n",
        "#         if extra_features:\n",
        "#             X = X.drop(columns=list(extra_features))\n",
        "\n",
        "#         # 정확한 순서로 특성 정렬\n",
        "#         X = X[required_features]\n",
        "\n",
        "#     # 사망률 예측\n",
        "#     try:\n",
        "#         mortality_prob = model.predict_proba(X)[0][1]\n",
        "#         mortality_label = 1 if mortality_prob >= 0.5 else 0\n",
        "#         print(f\"Patient {patient_id}: Mortality prediction = {mortality_label} (Probability: {mortality_prob:.2%})\")\n",
        "#         return mortality_prob, mortality_label\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error predicting for patient {patient_id}: {e}\")\n",
        "#         return None, None\n",
        "\n",
        "# # 3. 전체 데이터셋에 대한 특성 생성 함수 (원래 학습 파이프라인과 동일)\n",
        "# def prepare_dataset(data):\n",
        "#     \"\"\"\n",
        "#     원래 학습 파이프라인과 동일한 방식으로 전체 데이터셋에 대한 특성 생성\n",
        "#     \"\"\"\n",
        "#     print(\"Preparing dataset using original pipeline...\")\n",
        "\n",
        "#     # 1. aggregate_patient_data 함수 적용\n",
        "#     numerical_cols = [col for col in data.columns if col not in ['mortality_90d', 'icustayid', 'charttime', 'bloc']]\n",
        "#     agg_dict = {}\n",
        "\n",
        "#     # mortality_90d 열이 있으면 추가\n",
        "#     if 'mortality_90d' in data.columns:\n",
        "#         agg_dict['mortality_90d'] = 'first'\n",
        "\n",
        "#     # 수치형 컬럼에 대한 집계 설정\n",
        "#     for col in numerical_cols:\n",
        "#         if col == 'age':\n",
        "#             agg_dict[col] = 'first'\n",
        "#         else:\n",
        "#             agg_dict[col] = ['mean', 'min', 'max', 'std', 'last']\n",
        "\n",
        "#     # charttime 컬럼이 있으면 count 집계 추가\n",
        "#     if 'charttime' in data.columns:\n",
        "#         agg_dict['charttime'] = 'count'\n",
        "\n",
        "#     # 그룹화 및 집계\n",
        "#     patient_df = data.groupby('icustayid').agg(agg_dict)\n",
        "\n",
        "#     # 컬럼 이름 정리\n",
        "#     patient_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in patient_df.columns]\n",
        "\n",
        "#     # 2. clean_feature_names 함수 적용\n",
        "#     clean_df = patient_df.copy()\n",
        "#     clean_columns = {}\n",
        "#     for col in clean_df.columns:\n",
        "#         new_col = re.sub(r'[^\\w\\s_]', '', col)\n",
        "#         new_col = re.sub(r'\\s+', '_', new_col)\n",
        "#         clean_columns[col] = new_col\n",
        "\n",
        "#     patient_df = clean_df.rename(columns=clean_columns)\n",
        "\n",
        "#     # 3. X와 y 데이터 준비\n",
        "#     if 'mortality_90d_first' in patient_df.columns:\n",
        "#         X = patient_df.drop('mortality_90d_first', axis=1)\n",
        "#         y = patient_df['mortality_90d_first']\n",
        "#     else:\n",
        "#         X = patient_df\n",
        "#         y = None\n",
        "\n",
        "#     return X, y, patient_df\n",
        "\n",
        "# # 4. SHAP 값 계산 함수 (수정됨)\n",
        "# def analyze_shap_values(model, X_data, feature_names=None):\n",
        "#     \"\"\"\n",
        "#     KernelExplainer를 사용하여 SHAP 값을 계산하는 함수\n",
        "#     \"\"\"\n",
        "#     models = {'LightGBM': model}\n",
        "#     shap_values_dict = {}\n",
        "\n",
        "#     # 각 환자 ID별로 적절히 전처리된 데이터 준비\n",
        "#     if 'icustayid' in X_data.columns:\n",
        "#         print(\"Processing raw data with original pipeline...\")\n",
        "#         X_processed, _, _ = prepare_dataset(X_data)\n",
        "#     else:\n",
        "#         # 이미 전처리된 데이터라고 가정\n",
        "#         print(\"Using pre-processed data...\")\n",
        "#         X_processed = X_data\n",
        "\n",
        "#     # 샘플 수 제한 (메모리 및 성능 고려)\n",
        "#     n_samples = min(50, X_processed.shape[0])\n",
        "#     X_sample = X_processed.iloc[:n_samples]\n",
        "\n",
        "#     # feature_names 설정\n",
        "#     if feature_names is None:\n",
        "#         feature_names = X_processed.columns.tolist()\n",
        "\n",
        "#     for name, model in models.items():\n",
        "#         print(f\"Calculating SHAP values for {name} model...\")\n",
        "#         try:\n",
        "#             # 중요: predict_disable_shape_check=True 설정\n",
        "#             if hasattr(model, 'set_params'):\n",
        "#                 print(\"Setting predict_disable_shape_check=True\")\n",
        "#                 model.set_params(predict_disable_shape_check=True)\n",
        "\n",
        "#             # 먼저 TreeExplainer 시도\n",
        "#             try:\n",
        "#                 print(\"Trying SHAP TreeExplainer...\")\n",
        "#                 explainer = shap.TreeExplainer(model)\n",
        "#                 shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "#                 # 이진 분류 모델 처리\n",
        "#                 if isinstance(shap_values, list):\n",
        "#                     if len(shap_values) > 1:\n",
        "#                         shap_values = shap_values[1]  # 양성 클래스(사망)의 SHAP 값\n",
        "\n",
        "#                 print(\"TreeExplainer successful!\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"TreeExplainer failed: {e}\")\n",
        "#                 print(\"Switching to KernelExplainer...\")\n",
        "\n",
        "#                 # TreeExplainer 실패 시 KernelExplainer 사용\n",
        "#                 f = lambda x: model.predict_proba(x)[:, 1]\n",
        "#                 n_samples_kernel = min(20, X_sample.shape[0])  # 샘플 수 더 줄이기\n",
        "#                 X_kernel = X_sample.iloc[:n_samples_kernel]\n",
        "\n",
        "#                 explainer = shap.KernelExplainer(f, X_kernel)\n",
        "#                 shap_values = explainer.shap_values(X_kernel, nsamples=100)\n",
        "\n",
        "#                 # X_sample을 X_kernel로 업데이트\n",
        "#                 X_sample = X_kernel\n",
        "\n",
        "#             shap_values_dict[name] = {\n",
        "#                 'values': shap_values,\n",
        "#                 'explainer': explainer,\n",
        "#                 'data': X_sample\n",
        "#             }\n",
        "#             print(f\"Successfully calculated SHAP values for {name}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error calculating SHAP values for {name}: {e}\")\n",
        "\n",
        "#     # SHAP 값 계산 실패 시 대체 접근법\n",
        "#     if not shap_values_dict:\n",
        "#         print(\"WARNING: No SHAP values were calculated successfully\")\n",
        "#         print(\"Creating dummy SHAP data based on clinical knowledge\")\n",
        "\n",
        "#         # 더미 SHAP 데이터 생성\n",
        "#         shap_values_dict = create_meaningful_dummy_shap(model, X_sample, feature_names)\n",
        "\n",
        "#     return shap_values_dict, feature_names\n",
        "\n",
        "\n",
        "\n",
        "# # 6. 특정 환자의 SHAP 값 추출 함수\n",
        "# def extract_patient_shap_data(patient_id, X_data, shap_values_dict, feature_names):\n",
        "#     \"\"\"\n",
        "#     특정 환자의 SHAP 값을 추출하는 함수 (예외 처리 강화)\n",
        "#     \"\"\"\n",
        "#     # shap_values_dict가 비어 있는지 확인\n",
        "#     if not shap_values_dict:\n",
        "#         print(\"Error: SHAP values dictionary is empty\")\n",
        "#         return create_meaningful_dummy_shap_for_patient(patient_id, feature_names)\n",
        "\n",
        "#     # 주요 특성 목록 (SHAP 그래프에서 확인된 상위 10개)\n",
        "#     key_features = [\n",
        "#         'age_first', 'BUN_last', 'output_4hourly_std', 'SOFA_mean',\n",
        "#         'Weight_kg_mean', 'input_4hourly_last', 'mechvent_std',\n",
        "#         'GCS_mean', 'SIRS_mean', 'Creatinine_min'\n",
        "#     ]\n",
        "\n",
        "#     try:\n",
        "#         # 모델 이름 가져오기\n",
        "#         model_names = list(shap_values_dict.keys())\n",
        "#         if not model_names:\n",
        "#             print(\"Error: No models found in SHAP values dictionary\")\n",
        "#             return create_meaningful_dummy_shap_for_patient(patient_id, feature_names)\n",
        "\n",
        "#         model_name = model_names[0]\n",
        "#         shap_data = shap_values_dict[model_name]\n",
        "\n",
        "#         # SHAP 데이터에서 환자 인덱스 찾기\n",
        "#         X_sample = shap_data['data']\n",
        "\n",
        "#         # X_sample에 icustayid 열이 있는지 확인\n",
        "#         if 'icustayid' in X_sample.columns:\n",
        "#             patient_idx = X_sample[X_sample['icustayid'] == patient_id].index\n",
        "#             if len(patient_idx) == 0:\n",
        "#                 print(f\"Warning: Patient ID {patient_id} not found in SHAP sample data\")\n",
        "#                 # 대체 방법: 원본 데이터에서 환자 찾기\n",
        "#                 if 'icustayid' in X_data.columns:\n",
        "#                     orig_idx = X_data[X_data['icustayid'] == patient_id].index\n",
        "#                     if len(orig_idx) == 0:\n",
        "#                         print(f\"Error: Patient ID {patient_id} not found in any dataset\")\n",
        "#                         return create_meaningful_dummy_shap_for_patient(patient_id, feature_names)\n",
        "#                 # 샘플에 포함되지 않았으므로 첫 번째 샘플 사용 (데모 목적)\n",
        "#                 patient_idx = 0\n",
        "#             else:\n",
        "#                 patient_idx = patient_idx[0]\n",
        "#         else:\n",
        "#             # icustayid 열이 없는 경우, 첫 번째 샘플 사용 (데모 목적)\n",
        "#             print(\"Warning: icustayid column not found in SHAP sample data, using first sample\")\n",
        "#             patient_idx = 0\n",
        "\n",
        "#         # SHAP 값 추출\n",
        "#         shap_values = shap_data['values']\n",
        "\n",
        "#         # 환자 특성 및 SHAP 값 저장\n",
        "#         patient_info = {}\n",
        "\n",
        "#         for feature in key_features:\n",
        "#             # 특성 이름 매칭 (정확한 이름 또는 비슷한 이름)\n",
        "#             matched_feature = find_matching_feature(feature, feature_names)\n",
        "\n",
        "#             if matched_feature:\n",
        "#                 try:\n",
        "#                     feature_idx = feature_names.index(matched_feature)\n",
        "\n",
        "#                     # 특성값\n",
        "#                     feature_value = X_sample.iloc[patient_idx][matched_feature]\n",
        "\n",
        "#                     # SHAP 값\n",
        "#                     shap_value = shap_values[patient_idx, feature_idx]\n",
        "\n",
        "#                     # 결과 저장\n",
        "#                     patient_info[feature] = {\n",
        "#                         'value': float(feature_value),\n",
        "#                         'shap': float(shap_value),\n",
        "#                         'impact': 'positive' if shap_value > 0 else 'negative'\n",
        "#                     }\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error extracting SHAP data for feature {feature}: {e}\")\n",
        "#                     # 누락된 특성에 대한 대체 데이터 추가\n",
        "#                     patient_info[feature] = create_dummy_feature_data(feature)\n",
        "#             else:\n",
        "#                 print(f\"Warning: No matching feature found for {feature}\")\n",
        "#                 patient_info[feature] = create_dummy_feature_data(feature)\n",
        "\n",
        "#         # 전체 SHAP 값 합계 및 base_value\n",
        "#         try:\n",
        "#             base_value = shap_data['explainer'].expected_value\n",
        "#             if isinstance(base_value, list):\n",
        "#                 base_value = base_value[1]  # 이진 분류의 경우\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error extracting base_value: {e}\")\n",
        "#             base_value = 0.5  # 기본값\n",
        "\n",
        "#         try:\n",
        "#             total_shap = np.sum(shap_values[patient_idx])\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error calculating total SHAP: {e}\")\n",
        "#             total_shap = 0.0\n",
        "\n",
        "#         patient_info['base_value'] = float(base_value)\n",
        "#         patient_info['total_shap'] = float(total_shap)\n",
        "\n",
        "#         return patient_info\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error extracting patient SHAP data: {e}\")\n",
        "#         return create_meaningful_dummy_shap_for_patient(patient_id, feature_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # 10. Gemini API를 사용한 설명 생성 함수\n",
        "# def explain_mortality_with_gemini(patient_id, mortality_prob, patient_shap_data, api_key):\n",
        "#     \"\"\"\n",
        "#     Gemini API를 사용하여 환자의 사망률을 설명하는 함수\n",
        "#     \"\"\"\n",
        "#     # Gemini API 설정\n",
        "#     genai.configure(api_key=api_key)\n",
        "\n",
        "#     # 특성에 대한 설명 정보\n",
        "#     feature_descriptions = {\n",
        "#         'age_first': 'Patient\\'s age (first measurement)',\n",
        "#         'BUN_last': 'Blood Urea Nitrogen (BUN) level (last measurement)',\n",
        "#         'output_4hourly_std': 'Standard deviation of urine output measured every 4 hours',\n",
        "#         'SOFA_mean': 'Mean value of Sequential Organ Failure Assessment (SOFA) score',\n",
        "#         'Weight_kg_mean': 'Mean value of patient\\'s weight (kg)',\n",
        "#         'input_4hourly_last': 'Fluid intake measured every 4 hours (last measurement)',\n",
        "#         'mechvent_std': 'Standard deviation of mechanical ventilation usage',\n",
        "#         'GCS_mean': 'Mean value of Glasgow Coma Scale (GCS) score',\n",
        "#         'SIRS_mean': 'Mean value of Systemic Inflammatory Response Syndrome (SIRS) score',\n",
        "#         'Creatinine_min': 'Minimum value of creatinine level',\n",
        "#     }\n",
        "\n",
        "#     # 프롬프트 생성\n",
        "#     prompt = f\"\"\"\n",
        "#     You are an AI medical assistant that analyzes ICU patient medical data to predict and explain the probability of death within 90 days.\n",
        "#     Patient ID {patient_id} has a mortality probability of {mortality_prob:.1f}%.\n",
        "#     This prediction comes from a machine learning model based on SHAP (SHapley Additive exPlanations) values.\n",
        "#     SHAP values indicate whether each feature (patient's vital signs, clinical data, etc.) increases (positive) or decreases (negative) the risk of death.\n",
        "#     Below are the main feature values for this patient and their corresponding SHAP values:\n",
        "#     {json.dumps(patient_shap_data, indent=2)}\n",
        "#     Feature descriptions:\n",
        "#     {json.dumps(feature_descriptions, indent=2)}\n",
        "#     Based on the above data, please answer the following questions:\n",
        "\n",
        "#     What are the 3 most important factors increasing this patient's risk of death?\n",
        "#     What are the 3 most important factors decreasing this patient's risk of death?\n",
        "#     Explain medically why each factor has such an impact.\n",
        "#     Summarize this patient's overall health status and risk factors.\n",
        "#     Suggest what interventions or treatments might be effective in improving this patient's chances of survival.\n",
        "\n",
        "#     Please provide a professional response that doctors and nurses can understand, while also explaining clearly so that the patient's family can understand\n",
        "#     \"\"\"\n",
        "\n",
        "#     try:\n",
        "#         # Gemini 모델 설정 및 예측\n",
        "#         model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "#         response = model.generate_content(prompt)\n",
        "#         return response.text\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error generating explanation with Gemini: {e}\")\n",
        "#         return f\"\"\"\n",
        "#         환자 ID {patient_id}의 사망 확률은 {mortality_prob:.1f}%로 예측됩니다.\n",
        "\n",
        "#         Gemini API에서 오류가 발생하여 자세한 설명을 생성할 수 없습니다.\n",
        "#         오류 메시지: {str(e)}\n",
        "\n",
        "#         주요 특성과 SHAP 값을 참고하여 의료진과 상담하세요.\n",
        "#         \"\"\"\n",
        "\n",
        "# # 11. 종합 실행 함수\n",
        "# def mortality_explanation_agent(patient_id, data_path, model_path, gemini_api_key, visualize=False):\n",
        "#     \"\"\"\n",
        "#     환자 ID를 입력받아 사망률 예측과 설명을 제공하는 종합 에이전트\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # 1. 데이터 및 모델 로드\n",
        "#         data, model = load_data_and_model(data_path, model_path)\n",
        "\n",
        "#         # 2. 사망률 예측\n",
        "#         mortality_prob, mortality_label = predict_mortality_for_patient(patient_id, data, model)\n",
        "#         if mortality_prob is None:\n",
        "#             return f\"환자 ID {patient_id}의 사망률을 예측할 수 없습니다.\"\n",
        "\n",
        "#         # 3. 전체 데이터셋 전처리\n",
        "#         X_processed, y_processed, patient_df = prepare_dataset(data)\n",
        "\n",
        "#         # 4. SHAP 값 계산\n",
        "#         print(\"Calculating SHAP values...\")\n",
        "#         feature_names = X_processed.columns.tolist()\n",
        "#         shap_values_dict, feature_names = analyze_shap_values(model, X_processed, feature_names)\n",
        "\n",
        "#         # 5. 환자별 SHAP 값 추출\n",
        "#         patient_shap_data = extract_patient_shap_data(patient_id, data, shap_values_dict, feature_names)\n",
        "\n",
        "#         # 6. Gemini를 이용한 설명 생성\n",
        "#         explanation = explain_mortality_with_gemini(patient_id, mortality_prob * 100, patient_shap_data, gemini_api_key)\n",
        "\n",
        "#         return explanation\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Critical error in mortality_explanation_agent: {e}\")\n",
        "#         return f\"\"\"\n",
        "#         환자 ID {patient_id}에 대한 사망률 예측 과정에서 오류가 발생했습니다.\n",
        "#         오류 메시지: {str(e)}\n",
        "\n",
        "#         오류가 반복될 경우 시스템 관리자에게 문의하세요.\n",
        "#         \"\"\""
      ],
      "metadata": {
        "id": "_GhZrQ1w2KTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 메인 실행 코드\n",
        "# if __name__ == \"__main__\":\n",
        "#     # 설정값\n",
        "#     data_path = '/content/drive/MyDrive/project2/AI_agent_train_sepsis.csv'\n",
        "#     model_path = '/content/saved_models/lightgbm_model_weight4_leav100.pkl'\n",
        "#     gemini_api_key = 'AIzaSyDn1F2k4894S4bvT_SsPnSbH6cjt9Bu06U' # 실제 API 키로 교체\n",
        "\n",
        "#     # 환자 ID 입력 받기\n",
        "#     patient_id = int(input(\"Enter patient ID to analyze: \"))\n",
        "\n",
        "#     # 설명 생성\n",
        "#     explanation = mortality_explanation_agent(\n",
        "#         patient_id=patient_id,\n",
        "#         data_path=data_path,\n",
        "#         model_path=model_path,\n",
        "#         gemini_api_key=gemini_api_key\n",
        "#     )\n",
        "\n",
        "#     # 결과 출력\n",
        "#     print(\"\\n========== Mortality Explanation ==========\")\n",
        "#     print(explanation)\n",
        "#     print(\"===========================================\")"
      ],
      "metadata": {
        "id": "Hc_vdFT32a_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import google.generativeai as genai\n",
        "\n",
        "# 1. 데이터 및 모델 로드 함수\n",
        "def load_data_and_model(data_path, model_path):\n",
        "    \"\"\"\n",
        "    데이터와 모델을 파일에서 로드하는 함수\n",
        "    \"\"\"\n",
        "    # 데이터 로드\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    # 모델 로드\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    return data, model\n",
        "\n",
        "# 2. 특정 환자에 대한 사망률 예측 함수\n",
        "def predict_mortality_for_patient(patient_id, data, model):\n",
        "    \"\"\"\n",
        "    특정 환자에 대한 사망률 예측 함수 (원래 학습 파이프라인과 일치)\n",
        "    \"\"\"\n",
        "    # 환자 데이터 필터링\n",
        "    patient_data = data[data['icustayid'] == patient_id]\n",
        "\n",
        "    if len(patient_data) == 0:\n",
        "        print(f\"Error: Patient ID {patient_id} not found in the dataset\")\n",
        "        return None, None\n",
        "\n",
        "    # ====== 원래 학습 파이프라인과 동일한 전처리 적용 ======\n",
        "\n",
        "    # 1. aggregate_patient_data 함수 정확히 재현\n",
        "    numerical_cols = [col for col in patient_data.columns if col not in ['mortality_90d', 'icustayid', 'charttime', 'bloc']]\n",
        "    agg_dict = {}\n",
        "\n",
        "    # mortality_90d 열이 있으면 추가\n",
        "    if 'mortality_90d' in patient_data.columns:\n",
        "        agg_dict['mortality_90d'] = 'first'\n",
        "\n",
        "    # 수치형 컬럼에 대한 집계 설정\n",
        "    for col in numerical_cols:\n",
        "        if col == 'age':\n",
        "            agg_dict[col] = 'first'\n",
        "        else:\n",
        "            agg_dict[col] = ['mean', 'min', 'max', 'std', 'last']\n",
        "\n",
        "    # charttime 컬럼이 있으면 count 집계 추가\n",
        "    if 'charttime' in patient_data.columns:\n",
        "        agg_dict['charttime'] = 'count'\n",
        "\n",
        "    # 그룹화 및 집계\n",
        "    aggregated_data = patient_data.groupby('icustayid').agg(agg_dict)\n",
        "\n",
        "    # 컬럼 이름 정리\n",
        "    aggregated_data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in aggregated_data.columns]\n",
        "\n",
        "    # 2. clean_feature_names 함수 적용\n",
        "    clean_df = aggregated_data.copy()\n",
        "    clean_columns = {}\n",
        "    for col in clean_df.columns:\n",
        "        new_col = re.sub(r'[^\\w\\s_]', '', col)\n",
        "        new_col = re.sub(r'\\s+', '_', new_col)\n",
        "        clean_columns[col] = new_col\n",
        "\n",
        "    cleaned_data = clean_df.rename(columns=clean_columns)\n",
        "\n",
        "    # 3. 예측에 필요한 특성 준비 (target 변수 제외)\n",
        "    if 'mortality_90d_first' in cleaned_data.columns:\n",
        "        X = cleaned_data.drop('mortality_90d_first', axis=1)\n",
        "    else:\n",
        "        X = cleaned_data\n",
        "\n",
        "    # 모델이 기대하는 모든 특성 확인\n",
        "    if hasattr(model, 'feature_names_in_'):\n",
        "        required_features = model.feature_names_in_\n",
        "    elif hasattr(model, '_feature_name'):\n",
        "        required_features = model._feature_name  # LightGBM 모델\n",
        "    else:\n",
        "        required_features = []\n",
        "\n",
        "    # 누락된 특성 처리\n",
        "    if len(required_features) > 0:\n",
        "        missing_features = set(required_features) - set(X.columns)\n",
        "        print(f\"Missing {len(missing_features)} features\")\n",
        "\n",
        "        for feature in missing_features:\n",
        "            X[feature] = 0  # 누락된 특성에 기본값 설정\n",
        "\n",
        "        # 모델에 필요하지 않은 추가 특성 제거\n",
        "        extra_features = set(X.columns) - set(required_features)\n",
        "        if extra_features:\n",
        "            X = X.drop(columns=list(extra_features))\n",
        "\n",
        "        # 정확한 순서로 특성 정렬\n",
        "        X = X[required_features]\n",
        "\n",
        "    # 사망률 예측\n",
        "    try:\n",
        "        mortality_prob = model.predict_proba(X)[0][1]\n",
        "        mortality_label = 1 if mortality_prob >= 0.5 else 0\n",
        "        print(f\"Patient {patient_id}: Mortality prediction = {mortality_label} (Probability: {mortality_prob:.2%})\")\n",
        "        return mortality_prob, mortality_label, X  # X도 반환하도록 수정\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting for patient {patient_id}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# 3. 특정 환자의 SHAP 값 계산 함수\n",
        "def calculate_patient_specific_shap(patient_id, data, model):\n",
        "    \"\"\"\n",
        "    특정 환자에 대해서만 SHAP 값을 직접 계산하는 함수\n",
        "    \"\"\"\n",
        "    print(f\"Calculating SHAP values specifically for patient {patient_id}...\")\n",
        "\n",
        "    # 1. 환자 데이터 전처리 및 예측\n",
        "    mortality_prob, mortality_label, X = predict_mortality_for_patient(patient_id, data, model)\n",
        "\n",
        "    if X is None:\n",
        "        print(f\"Error: Failed to process data for patient {patient_id}\")\n",
        "        return None\n",
        "\n",
        "    # 2. 특성 이름 목록 가져오기\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # 3. SHAP 값 계산\n",
        "    try:\n",
        "        # predict_disable_shape_check=True 설정\n",
        "        if hasattr(model, 'set_params'):\n",
        "            print(\"Setting predict_disable_shape_check=True\")\n",
        "            model.set_params(predict_disable_shape_check=True)\n",
        "\n",
        "        # TreeExplainer 생성\n",
        "        print(\"Creating SHAP TreeExplainer...\")\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "\n",
        "        # 환자 데이터에 대한 SHAP 값 계산\n",
        "        print(f\"Calculating SHAP values for patient {patient_id}...\")\n",
        "        shap_values = explainer.shap_values(X)\n",
        "\n",
        "        # 이진 분류 모델 처리\n",
        "        if isinstance(shap_values, list):\n",
        "            if len(shap_values) > 1:\n",
        "                print(\"Using positive class SHAP values (index 1)\")\n",
        "                shap_values = shap_values[1]  # 양성 클래스(사망)의 SHAP 값\n",
        "            else:\n",
        "                print(\"Using available SHAP values (index 0)\")\n",
        "                shap_values = shap_values[0]\n",
        "\n",
        "        # 4. 주요 특성 목록\n",
        "        key_features = [\n",
        "            'age_first', 'BUN_last', 'output_4hourly_std', 'SOFA_mean',\n",
        "            'Weight_kg_mean', 'input_4hourly_last', 'mechvent_std',\n",
        "            'GCS_mean', 'SIRS_mean', 'Creatinine_min'\n",
        "        ]\n",
        "\n",
        "        # 5. 결과 저장\n",
        "        patient_info = {}\n",
        "\n",
        "        for feature in key_features:\n",
        "            # 특성 이름 매칭 (정확한 이름 또는 비슷한 이름)\n",
        "            matched_feature = find_matching_feature(feature, feature_names)\n",
        "\n",
        "            if matched_feature:\n",
        "                try:\n",
        "                    feature_idx = feature_names.index(matched_feature)\n",
        "\n",
        "                    # 특성값\n",
        "                    feature_value = X[matched_feature].values[0]\n",
        "\n",
        "                    # SHAP 값\n",
        "                    shap_value = shap_values[0, feature_idx]\n",
        "\n",
        "                    # 결과 저장\n",
        "                    patient_info[feature] = {\n",
        "                        'value': float(feature_value),\n",
        "                        'shap': float(shap_value),\n",
        "                        'impact': 'positive' if shap_value > 0 else 'negative'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting SHAP data for feature {feature}: {e}\")\n",
        "                    # 특성을 찾지 못한 경우 메시지만 출력\n",
        "                    print(f\"Feature {feature} not found or cannot calculate SHAP value\")\n",
        "            else:\n",
        "                print(f\"Warning: No matching feature found for {feature}\")\n",
        "\n",
        "        # 전체 SHAP 값 합계 및 base_value\n",
        "        try:\n",
        "            base_value = explainer.expected_value\n",
        "            if isinstance(base_value, list):\n",
        "                base_value = base_value[1]  # 이진 분류의 경우\n",
        "            patient_info['base_value'] = float(base_value)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting base_value: {e}\")\n",
        "            patient_info['base_value'] = 0.5  # 기본값\n",
        "\n",
        "        try:\n",
        "            total_shap = np.sum(shap_values[0])\n",
        "            patient_info['total_shap'] = float(total_shap)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating total SHAP: {e}\")\n",
        "            patient_info['total_shap'] = 0.0\n",
        "\n",
        "        print(f\"Successfully calculated SHAP values for patient {patient_id}\")\n",
        "        return patient_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating SHAP values for patient {patient_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# 4. 특성 이름 매칭 함수\n",
        "def find_matching_feature(target_feature, feature_names):\n",
        "    \"\"\"\n",
        "    특성 이름 매칭 함수 (정확한 이름 또는 비슷한 이름)\n",
        "    \"\"\"\n",
        "    # 정확히 일치하는 경우\n",
        "    if target_feature in feature_names:\n",
        "        return target_feature\n",
        "\n",
        "    # 부분 일치 찾기\n",
        "    target_parts = target_feature.lower().split('_')\n",
        "\n",
        "    for feature in feature_names:\n",
        "        feature_lower = feature.lower()\n",
        "        match_score = 0\n",
        "\n",
        "        for part in target_parts:\n",
        "            if part in feature_lower:\n",
        "                match_score += 1\n",
        "\n",
        "        # 2개 이상의 부분이 일치하면 매칭으로 간주\n",
        "        if match_score >= 2:\n",
        "            return feature\n",
        "\n",
        "    return None\n",
        "\n",
        "# 5. Gemini API를 사용한 설명 생성 함수\n",
        "def explain_mortality_with_gemini(patient_id, mortality_prob, patient_shap_data, api_key):\n",
        "    \"\"\"\n",
        "    Gemini API를 사용하여 환자의 사망률을 설명하는 함수\n",
        "    \"\"\"\n",
        "    # Gemini API 설정\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    # 특성에 대한 설명 정보\n",
        "    feature_descriptions = {\n",
        "        'age_first': 'Patient\\'s age (first measurement)',\n",
        "        'BUN_last': 'Blood Urea Nitrogen (BUN) level (last measurement)',\n",
        "        'output_4hourly_std': 'Standard deviation of urine output measured every 4 hours',\n",
        "        'SOFA_mean': 'Mean value of Sequential Organ Failure Assessment (SOFA) score',\n",
        "        'Weight_kg_mean': 'Mean value of patient\\'s weight (kg)',\n",
        "        'input_4hourly_last': 'Fluid intake measured every 4 hours (last measurement)',\n",
        "        'mechvent_std': 'Standard deviation of mechanical ventilation usage',\n",
        "        'GCS_mean': 'Mean value of Glasgow Coma Scale (GCS) score',\n",
        "        'SIRS_mean': 'Mean value of Systemic Inflammatory Response Syndrome (SIRS) score',\n",
        "        'Creatinine_min': 'Minimum value of creatinine level',\n",
        "    }\n",
        "\n",
        "    # 프롬프트 생성\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI medical assistant that analyzes ICU patient medical data to predict and explain the probability of death within 90 days.\n",
        "    Patient ID {patient_id} has a mortality probability of {mortality_prob:.1f}%.\n",
        "    This prediction comes from a machine learning model based on SHAP (SHapley Additive exPlanations) values.\n",
        "    SHAP values indicate whether each feature (patient's vital signs, clinical data, etc.) increases (positive) or decreases (negative) the risk of death.\n",
        "    Below are the main feature values for this patient and their corresponding SHAP values:\n",
        "    {json.dumps(patient_shap_data, indent=2)}\n",
        "    Feature descriptions:\n",
        "    {json.dumps(feature_descriptions, indent=2)}\n",
        "    Based on the above data, please answer the following questions:\n",
        "\n",
        "    What are the 3 most important factors increasing this patient's risk of death?\n",
        "    What are the 3 most important factors decreasing this patient's risk of death?\n",
        "    Explain medically why each factor has such an impact.\n",
        "    Summarize this patient's overall health status and risk factors.\n",
        "    Suggest what interventions or treatments might be effective in improving this patient's chances of survival.\n",
        "\n",
        "    Please provide a professional response that doctors and nurses can understand, while also explaining clearly so that the patient's family can understand\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Gemini 모델 설정 및 예측\n",
        "        model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating explanation with Gemini: {e}\")\n",
        "        return f\"\"\"\n",
        "        환자 ID {patient_id}의 사망 확률은 {mortality_prob:.1f}%로 예측됩니다.\n",
        "\n",
        "        Gemini API에서 오류가 발생하여 자세한 설명을 생성할 수 없습니다.\n",
        "        오류 메시지: {str(e)}\n",
        "\n",
        "        주요 특성과 SHAP 값을 참고하여 의료진과 상담하세요.\n",
        "        \"\"\"\n",
        "\n",
        "# 6. 종합 실행 함수\n",
        "def mortality_explanation_agent(patient_id, data_path, model_path, gemini_api_key):\n",
        "    \"\"\"\n",
        "    환자 ID를 입력받아 사망률 예측과 설명을 제공하는 종합 에이전트\n",
        "    (특정 환자에 대한 SHAP 값 직접 계산 방식 사용)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. 데이터 및 모델 로드\n",
        "        data, model = load_data_and_model(data_path, model_path)\n",
        "\n",
        "        # 2. 사망률 예측\n",
        "        mortality_prob, mortality_label, _ = predict_mortality_for_patient(patient_id, data, model)\n",
        "        if mortality_prob is None:\n",
        "            return f\"환자 ID {patient_id}의 사망률을 예측할 수 없습니다.\"\n",
        "\n",
        "        # 3. 특정 환자에 대한 SHAP 값 직접 계산\n",
        "        print(\"Calculating SHAP values specifically for this patient...\")\n",
        "        patient_shap_data = calculate_patient_specific_shap(patient_id, data, model)\n",
        "\n",
        "        if patient_shap_data is None:\n",
        "            return f\"환자 ID {patient_id}의 SHAP 값을 계산할 수 없습니다.\"\n",
        "\n",
        "        # 4. Gemini를 이용한 설명 생성\n",
        "        explanation = explain_mortality_with_gemini(patient_id, mortality_prob * 100, patient_shap_data, gemini_api_key)\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error in mortality_explanation_agent: {e}\")\n",
        "        return f\"\"\"\n",
        "        환자 ID {patient_id}에 대한 사망률 예측 과정에서 오류가 발생했습니다.\n",
        "        오류 메시지: {str(e)}\n",
        "\n",
        "        오류가 반복될 경우 시스템 관리자에게 문의하세요.\n",
        "        \"\"\"\n",
        "\n",
        "# 메인 실행 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 설정값\n",
        "    data_path = '/content/drive/MyDrive/project2/AI_agent_train_sepsis.csv'\n",
        "    model_path = '/content/saved_models/lightgbm_model_weight4_leav100.pkl'\n",
        "    gemini_api_key = 'AIzaSyDn1F2k4894S4bvT_SsPnSbH6cjt9Bu06U' # 실제 API 키로 교체\n",
        "\n",
        "    # 환자 ID 입력 받기\n",
        "    patient_id = int(input(\"Enter patient ID to analyze: \"))\n",
        "\n",
        "    # 설명 생성\n",
        "    explanation = mortality_explanation_agent(\n",
        "        patient_id=patient_id,\n",
        "        data_path=data_path,\n",
        "        model_path=model_path,\n",
        "        gemini_api_key=gemini_api_key\n",
        "    )\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"\\n========== Mortality Explanation ==========\")\n",
        "    print(explanation)\n",
        "    print(\"===========================================\")"
      ],
      "metadata": {
        "id": "SpoGrDh44FP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}